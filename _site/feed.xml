<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Park's Archive</title>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <link>https://jhyun0919.github.io//</link>
    <description>A website with blog posts and pages</description>
    <pubDate>Sun, 27 Oct 2019 23:06:53 -0500</pubDate>
    
      <item>
        <title>(TF vs. PyTorch) MNIST tutorial</title>
        <link>/research/ml&dl/2019/10/22/mnist.html</link>
        <guid isPermaLink="true">/research/ml&dl/2019/10/22/mnist.html</guid>
        <description>&lt;p&gt;MNIST is the set of data for training the machine to learn handwritten numeral images, which is the most popular and appropriate subject for the purpose of entering deep learning.&lt;/p&gt;

&lt;p&gt;Through this post, piece of codes with explanation will be provided and full codes are upload on the following links;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jhyun0919/deep_dive_into_tensorflow/blob/master/exercise/fastcampus/mnist_explained.ipynb&quot;&gt;MNIST code-TF ver.ipynb&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jhyun0919/deep_dive_into_pytorch/blob/master/exercise/fast_campus/pytorch_mnist_explained.ipynb&quot;&gt;MNIST code-PyTorch ver.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also, this post is written with reference to the following sources;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.tensorflow.org/tutorials/quickstart/advanced&quot;&gt;Tensorflow 2 quickstart for experts&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py&quot;&gt;PyTorch &amp;gt; Tutorials &amp;gt; Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://online.fastcampus.co.kr/p/data_online_deep&quot;&gt;FastCampus &amp;gt; DeepLearningLecture&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;tensorflow-version&quot;&gt;Tensorflow version&lt;/h1&gt;
&lt;h2 id=&quot;eda-exploratory-data-analysis&quot;&gt;EDA (Exploratory Data Analysis)&lt;/h2&gt;
&lt;p&gt;In order to train a deep learning model, the first thing to do is to explore and analyze the given dataset. In this stage, we can get some hints for designing a structure of the model. The things we have to check in EDA are following;
First we need to check size and shape of feature data and target data. Based on the result, we can confirm that a proper shape of input &amp;amp; output data for our model; Input data shape is 28 by 28 matrix(or tensor) and output is a scalar.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;script src=&quot;https://gist.github.com/jhyun0919/05ee6cbb224ee868d324648a8a9a9641.js&quot;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;Second, we we have to check the value of target data. Since the target data is discrete, we can confirm that the model will be a classification model, and we will refer target as label from now on. 
Also, we need to check that the distribution of labels in train and test dataset is not biased. If dataset is biased, the model trained with this dataset will also biased. This is one of the main reason why we need to check and analyze the given dataset before build and train the model.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;script src=&quot;https://gist.github.com/jhyun0919/f262d68508dd323dd06c3e69a08c5cb0.js&quot;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;Check distribution of labelsDistribution of label dataset(Optional) Check the real image of feature data. Checking how the feature data looks like is not necessary in training and evaluating the model. However, this will allows you intuitive understanding for analyzing dataset and designing a structure of the model.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;script src=&quot;https://gist.github.com/jhyun0919/64fcc0c48751abad1f4553241581a744.js&quot;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;p&gt;Plot a real image of feature dataSample image of feature data&lt;/p&gt;

&lt;h2 id=&quot;data-preprocess&quot;&gt;Data preprocess&lt;/h2&gt;
&lt;p&gt;First, the given dataset need to be reshaped properly. In this step, feature data need an additional dimension to use convolution layers. Also, label data need to be reshaped label encoding into one-hot-encoding for a classification model [1].
Second, feature data have to be normalized [2]. Through this process, all features data are scaled to a same unit preventing from one feature dominate others.
Lastly, we will use batch to control stability of the training [3]. Also, data distribution of each batches might occur bias, so train data need to be shuffled before split into batches. It’s worth noting at this step that Tensorflow provides a nice feature called tf.Data to help organize the given data.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;script src=&quot;https://gist.github.com/jhyun0919/a581e5d32df2034ee7507847e2c0d9da.js&quot;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;
&lt;p&gt;The model is comprised of two parts, feature extraction and classification. In the feature extraction stage, the model use two dimension convolution filter (Conv2D) since the input feature data is image data. Addition to Conv2D, rectified linear unit (ReLU), pooling and dropout unit [4] are used. In classification stage, a fully connected network (FCN) is used. Also, softmax function is attached at the end of the FCN which give us probabilities for each class label.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;script src=&quot;https://gist.github.com/jhyun0919/3ca0add7088a59a2707075b3b30e8419.js&quot;&gt;&lt;/script&gt;
&lt;/p&gt;

&lt;h2 id=&quot;optional-explanation-of-the-modelslayers&quot;&gt;(Optional) Explanation of the model’s layers&lt;/h2&gt;
&lt;h3 id=&quot;conv2d&quot;&gt;Conv2D&lt;/h3&gt;

&lt;h3 id=&quot;activation&quot;&gt;Activation&lt;/h3&gt;

&lt;h3 id=&quot;pooling&quot;&gt;Pooling&lt;/h3&gt;

&lt;h3 id=&quot;fully-connected&quot;&gt;Fully Connected&lt;/h3&gt;

&lt;h2 id=&quot;graph&quot;&gt;Graph&lt;/h2&gt;
&lt;p&gt;Training a model requires proper loss object and optimizer.&lt;/p&gt;

&lt;h2 id=&quot;train-evaluate&quot;&gt;Train &amp;amp; Evaluate&lt;/h2&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;pytorch-version&quot;&gt;PyTorch version&lt;/h1&gt;
&lt;h2 id=&quot;eda&quot;&gt;EDA&lt;/h2&gt;
&lt;p&gt;EDA is skipped because it is similar to the process performed using Tensorflow.&lt;/p&gt;

&lt;h2 id=&quot;data-preprocess-1&quot;&gt;Data preprocess&lt;/h2&gt;

&lt;h2 id=&quot;model-1&quot;&gt;Model&lt;/h2&gt;

&lt;h2 id=&quot;optional-explanation-of-the-modelslayers-1&quot;&gt;(Optional) Explanation of the model’s layers&lt;/h2&gt;
&lt;h3 id=&quot;conv2d-1&quot;&gt;Conv2d&lt;/h3&gt;

&lt;h3 id=&quot;pooling-1&quot;&gt;Pooling&lt;/h3&gt;

&lt;h3 id=&quot;fully-connected-1&quot;&gt;Fully Connected&lt;/h3&gt;

&lt;h2 id=&quot;train-evaluate-1&quot;&gt;Train &amp;amp; Evaluate&lt;/h2&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] A. J. SpiderCloud Wireless, “Why using one-hot encoding for training classifier,” LinkedIn. [Online]. Available: https://www.linkedin.com/pulse/why-using-one-hot-encoding-classifier-training-adwin-jahn. [Accessed: 26-Oct-2019].&lt;/p&gt;

&lt;p&gt;[2] U. Jaitley, “Why Data Normalization is necessary for Machine Learning models,” Medium, 09-Apr-2019. [Online]. Available: https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029. [Accessed: 26-Oct-2019].&lt;/p&gt;

&lt;p&gt;[3] J. Brownlee, “How to Control the Stability of Training Neural Networks With the Batch Size,” Machine Learning Mastery, 03-Oct-2019. [Online]. Available: https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/. [Accessed: 26-Oct-2019].&lt;/p&gt;

&lt;p&gt;[4] Hinton, G., Srivastava, N., Krizhevsky, A., Sutskever, I. and Salakhutdinov, R. (2019). Improving neural networks by preventing co-adaptation of feature detectors. [online] arXiv.org. Available at: https://arxiv.org/abs/1207.0580 [Accessed 26 Oct. 2019].&lt;/p&gt;

&lt;p&gt;[5] A. S. V, “Understanding Activation Functions in Neural Networks,” Medium, 30-Mar-2017. [Online]. Available: https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0. [Accessed: 26-Oct-2019].&lt;/p&gt;

&lt;p&gt;[6] “Pooling,” Unsupervised Feature Learning and Deep Learning Tutorial. [Online]. Available: http://deeplearning.stanford.edu/tutorial/supervised/Pooling/. [Accessed: 26-Oct-2019].&lt;/p&gt;
</description>
        <pubDate>Tue, 22 Oct 2019 00:00:00 -0500</pubDate>
      </item>
    
      <item>
        <title>(TF vs. PyTorch) Introduction of becoming adept in deep learning frameworks</title>
        <link>/research/ml&dl/2019/10/21/deeplearning_framework.html</link>
        <guid isPermaLink="true">/research/ml&dl/2019/10/21/deeplearning_framework.html</guid>
        <description>&lt;p&gt;Due to rapid improvements in computing performance and the amount of data that is accumulated, deep learning is gaining strength. Accordingly, IT Giants develop their deep learning framework to provide developers with a development environment, such as Google’s Tensorflow and Facebook’s PyTorch.&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://jhyun0919.github.io/assets/img//2019-10-21-01.png&quot; width=&quot;600&quot; /&gt;
  &lt;figcaption&gt;Figure 1. Deep learning frameworks&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Each framework has its characteristics and strength, and they are properly used for appropriate purposes. For engineers with research purposes, it is necessary to note Figure 2. According to data from RISELab, the recent trend in the papers uploaded on arXiv.org shows that TensorFlow and PyTorch are mainly used in research purposes. It seems that choosing Tensorflow or PyTorch would be the best choice for people who are planning to dive in deep learning. It will allow understanding other’s research more easily and quickly.&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://jhyun0919.github.io/assets/img//2019-10-21-02.png&quot; width=&quot;800&quot; /&gt;
  &lt;figcaption&gt;Figure 2. The number of papers posted on arXiv.org that mention each framework. Source: Data from RISELab and graphic by Ben Lorica.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In the future, I will select various topics and upload a series of tutorials, and code will also be uploaded in two versions, Tensorflow and PyTorch. First, I will upload a post about basic topics, such as the MNIST tutorial. After that, I will discuss energy-related time series and reinforcement learning topics, which are my research interests. I hope this will help engineers who are new to using deep learning field or would like to be more adept at dealing with the frameworks.&lt;/p&gt;
</description>
        <pubDate>Mon, 21 Oct 2019 00:00:00 -0500</pubDate>
      </item>
    
      <item>
        <title>What makes me nervous these days</title>
        <link>/daily/essay/2019/09/22/essay.html</link>
        <guid isPermaLink="true">/daily/essay/2019/09/22/essay.html</guid>
        <description>&lt;p&gt;After Two years of working as a researcher at Korea Electronics Technolog Institute, I entered the University of Texas at Austin(UT Austin) as a master’s student. During four months, since I have got an admission offer from UT Austin, people showed different reactions. Some celebrated about entering UT Austin, and others were worried about studying abroad. Former voices, which were louder til I left my home town, become smaller and I got nervous.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://media.giphy.com/media/LytiZGHa3DbCE/giphy.gif&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Everything was going as I planned. The UT Cockrell School of Engineering was one of my dream school, and my registration was also well on it’s well. I had a great time during International Student OT and Department wise OT. Also, I added all the courses I wanted for my first semester. I thought there was no reason for getting nervous or anxious. However, I cannot ignore my anxiety, and it was getting worse as time went by.
So I changed my strategy. I started to find out what made me nervous. After a week of figuring out, I found the most plausible reason. It was because of me being impatient with success.It was just a starting point of two years journey. The worst part was that I left the issue after I realized the problem since I did not know how to deal with it.
While I was getting tired of this feeling, I read confession of one man which changed my mind. He was a man who just got discharged from the army and trying to settle in where he came from. He said, “I was nervous because I was doing nothing at that time.” At that moment, the words penetrated my heart and made me think, “I should do something, even if it looks not meaningful.” And my answer was to start recording my experience in writing. I believe it allows me to look back at myself and gives me an opportunity to do better next time.&lt;/p&gt;

&lt;p&gt;This writing is my first step to get rid of my anxiety and improve my school life. Although my first post in UT Austin starts with unfavorable story, I will end up my writing with good news about myself.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://media.giphy.com/media/pOZhmE42D1WrCWATLK/giphy.gif&quot; width=&quot;600&quot; /&gt;
&lt;/p&gt;
</description>
        <pubDate>Sun, 22 Sep 2019 00:00:00 -0500</pubDate>
      </item>
    
  </channel>
</rss>
