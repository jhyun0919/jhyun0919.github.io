<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Park's Archive</title>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <link>http://localhost:4000/</link>
    <description>A website with blog posts and pages</description>
    <pubDate>Sat, 17 Apr 2021 16:35:58 -0500</pubDate>
    
      <item>
        <title>(eitingüë∑üèª‚Äç‚ôÇÔ∏è) Dr. Zhu's Group Meeting Log</title>
        <link>/research/meeting%20log/2021/04/16/Dr-Zhu-Group-Meeting-Log.html</link>
        <guid isPermaLink="true">/research/meeting%20log/2021/04/16/Dr-Zhu-Group-Meeting-Log.html</guid>
        <description>&lt;h3 id=&quot;speech-title-grid-aware-machine-learning-for-distribution-system-modeling-monitoring-and-optimization&quot;&gt;Speech Title: Grid-Aware Machine Learning for Distribution System Modeling, Monitoring, and Optimization&lt;/h3&gt;

&lt;hr /&gt;

&lt;p&gt;The contents covered in this article are credited to the speaker of the meeting, Dr. Zhu, and her research group members.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;tl-dr&quot;&gt;TL; DR&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;speaker&quot;&gt;Speaker&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Shanny Lin&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Abstract or Short Brief&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;keywords&quot;&gt;Keywords&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;modeling, monitoring, optimization&lt;/li&gt;
  &lt;li&gt;partial observability&lt;/li&gt;
  &lt;li&gt;CVaR&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;group-meeting-recap&quot;&gt;Group Meeting Recap&lt;/h2&gt;

&lt;h3 id=&quot;motivation&quot;&gt;Motivation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;problem-formulation&quot;&gt;Problem Formulation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;How to estimate an accurate model of the power system?&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;How to address monitoring and optimization tasks under limited observability? &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;proposed-approach&quot;&gt;Proposed Approach&lt;/h3&gt;

&lt;p&gt;&lt;a name=&quot;Eq. (2)&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;modeling&quot;&gt;Modeling&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Linearized distribution flow model&lt;/li&gt;
&lt;/ul&gt;

\[\begin{aligned}
\mathbf{v} &amp;amp; \approx \mathbf{Rp} + \mathbf{Xq} \\
&amp;amp; = \mathbf{M}^{-T}\mathbf{D}_{r}\mathbf{M}^{-1}\mathbf{p} + \mathbf{M}^{-T}\mathbf{D}_{x}\mathbf{M}^{-1}\mathbf{q}
\end{aligned}
\tag{1}\]

&lt;ul&gt;
  &lt;li&gt;Bi-linear regression with &lt;a href=&quot;#Group-LASSO&quot;&gt;Group-LASSO regularization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

\[\min_{\mathbf{\theta}, \{\tilde{\mathbf{s}}^{\mathcal{U}}_{t}\}}{\sum_{t}^{T}{\| \tilde{\mathbf{v}}_t} - \mathbf{A}(\tilde{\mathbf{s}}^{\mathcal{O}}_{t})\mathbf{\theta} - \mathbf{A}(\tilde{\mathbf{s}}^{\mathcal{U}}_{t})\mathbf{\theta} \|_2^2 + \lambda \| \tilde{\mathbf{s}}^{\mathcal{U}}_{t} \|_G}
\tag{2}\]

&lt;ul&gt;
  &lt;li&gt;Alternating Minimization (AM)&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;between $\mathbf{\theta}$ and ${\tilde{\mathbf{s}}^{\mathcal{U}}_{t}}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;monitoring&quot;&gt;Monitoring&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;DER visibility by leveraging heterogenous &amp;amp; dynamic data.&lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;Smart meter data ($\mathbf{\Gamma}$)&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
      &lt;li&gt;D-PMU data ($\mathbf{Z}$)&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Spatio-temporal learning&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

\[\begin{aligned}
\begin{bmatrix} \mathbf{P} \\ \mathbf{Q} \end{bmatrix} &amp;amp;= \mathbf{L} + \mathbf{DU} \\
&amp;amp; = \mathbf{L} + \begin{bmatrix} \mathbf{D}^{P} \\ \mathbf{D}^{Q} \end{bmatrix}
\end{aligned}
\tag{3}\]

\[\begin{aligned}
\min&amp;amp;_{\mathbf{L}, \mathbf{D}}{\|\mathbf{L}\|_{*} + \lambda\|\mathbf{D}\|_{G}} \\
&amp;amp; \text{s.to } \mathbf{L} + \mathbf{D}\mathbf{U} \text{ satisfies error bound for } \mathbf{\Gamma} \text{ and } \mathbf{Z}
\end{aligned}
\tag{4}\]

\[\begin{aligned}
\min&amp;amp;_{\mathbf{v}, \mathbf{D}}{\frac{1}{2} \|\mathbf{v}\|_{2}^{2} + \lambda\|\mathbf{D}\|_{G}}\\
&amp;amp; \text{s.to } \mathbf{uv^{T}} + \mathbf{D}\mathbf{U} \text{ satisfies error bound for } \mathbf{\Gamma} \text{ and } \mathbf{Z}
\end{aligned}
\tag{5}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;optimization&quot;&gt;Optimization&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;DER optimization&lt;/li&gt;
&lt;/ul&gt;

\[\begin{aligned}
\mathbf{\hat{q}} = &amp;amp; \min_{\mathbf{q} \in \mathcal{Q}}{Losses(\mathbf{q})} \\
&amp;amp; \text{s.to } \begin{bmatrix} \mathbf{Xq} + \mathbf{y} - \mathbf{\bar{v}} \\ -\mathbf{Xq} - \mathbf{y} + \mathbf{\underline{v}} \end{bmatrix} \leq\mathbf{0}
\end{aligned}
\tag{6}\]

&lt;ul&gt;
  &lt;li&gt;Graph learning&lt;/li&gt;
&lt;/ul&gt;

\[\min_{\Phi}{Avg(\| \Phi(\mathbf{z}) -  \mathbf{\hat{q}}\|_{2}^{2})}
\tag{7}\]

\[\mathbf{z}_{l+1} = \sigma(\mathbf{W}_l\mathbf{z}_l\mathbf{H}_l + \mathbf{b}_l)
\tag{8}\]

&lt;ul&gt;
  &lt;li&gt;Conditional Value-at-Risk (CVaR) method.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;experiment--result&quot;&gt;Experiment &amp;amp; Result&lt;/h3&gt;

&lt;h4 id=&quot;modeling-1&quot;&gt;Modeling&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Line reactance estimation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;monitoring-1&quot;&gt;Monitoring&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;DER visibility&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;optimization-1&quot;&gt;Optimization&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Graph learning vs. Local learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;future-study&quot;&gt;Future Study&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;what-i-benefit&quot;&gt;What I Benefit&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;Group-LASSO&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;regularization&quot;&gt;Regularization&lt;/h3&gt;

&lt;h4 id=&quot;lasso-and-ridge-regressions-&quot;&gt;Lasso and Ridge Regressions []&lt;/h4&gt;

&lt;p&gt;Given a dataset ${X,y}$ where $X$ is the feature and $y$ is the label for regression, we simply model it as has a linear relationship $y = X\beta$. With regularization, we can control the degree of freedom of the model parameter ($\beta$) and able to avoid the risk of overfitting.&lt;/p&gt;

&lt;p&gt;The two representative regularizations are LASSO (L1) and Ridge (L2), and they are defined as follows.&lt;/p&gt;

\[\beta^{*} = \underset{\beta}{\mathrm{argmin}} {\| y - X\beta \|_2^2} + \lambda \| \beta \|_{1}\]

\[\beta^{*} = \underset{\beta}{\mathrm{argmin}} {\| y - X\beta \|_2^2} + \lambda \| \beta \|_{2}\]

&lt;p&gt;Due to the shape of their constraints boundary (norm ball shape), LASSO encourage a sparse result as a solution of the optimal problem.&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-04-16-Dr Zhu Group Meeting Log/lasso-vs-ridge.png&quot; width=&quot;500&quot; /&gt;   &lt;figcaption&gt;
Figure . Conventional Explanation to Sparsity Caused by Lasso. []
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;group-lasso&quot;&gt;Group LASSO&lt;/h4&gt;

&lt;p&gt;Suppose the weights in $\beta$ could be grouped, the new weight vector becomes $\beta_G = { \beta^{(1)}, \beta^{(2)},‚ãØ,\beta^{(m)} }$. Each $\beta^{(l)}$ for $1 \leq l \leq m$ represents a group of weights from $\beta$.&lt;/p&gt;

&lt;p&gt;We further group $X$ accordingly. We denote $X(l)$ as the submatrix of $X$ with columns corresponding to the weights in $\beta^{(l)}$. The optimization problem becomes&lt;/p&gt;

\[\beta^{*} = \underset{\beta}{\mathrm{argmin}} {\| y - \sum_{l=1}^{m}{X^{(l)}\beta^{(l)}} \|_2^2} + \lambda \sum_{l=1}^{m} \sqrt{p_{l}}\| \beta^{(l)} \|_{1}\]

&lt;p&gt;where $p^{(l)}$ represents the number of weights in $\beta^{(l)}$.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;why-do-we-need-grouping&quot;&gt;Why do we need GROUPING?&lt;/h4&gt;

&lt;p&gt;Usually, the variables of the minimization problem are not correlated. However, when there are certain constraints or correlation between variables, we need to group them appropriately.&lt;/p&gt;

&lt;p&gt;The power injections as minimization variables, covered in the presentation , is a good example. Since power is comprised of active and reactive power, the variable of &lt;a href=&quot;#Eq. (2)&quot;&gt;the minimization problem of Eq. (2)&lt;/a&gt; has to be grouped as follows.&lt;/p&gt;

\[\tilde{\mathbf{s}}_{t}\ = \begin{bmatrix} \tilde{p}_n &amp;amp; \tilde{q}_n \end{bmatrix}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;cvar&quot;&gt;CVaR&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;throw-back-the-question&quot;&gt;Throw back the Question&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;what-i-contribute&quot;&gt;What I Contribute&lt;/h2&gt;

&lt;h3 id=&quot;the-framework-of-researchstudy&quot;&gt;The framework of research/study&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Motivation  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Problem Formulation  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Proposed Approach  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Experiment &amp;amp; Result  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Future Study  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The above list is the framework of research that Dr. Zhu discussed at the start of this year. Almost all research papers have been written along this process, which is also the basic step of our research.&lt;/p&gt;

&lt;p&gt;However, it was not easy for me to summarize the contents of today‚Äôs meeting with this framework since there are three main topics in a single speech. From my point of view, one of each topic is good enough for one individual presentation, and it would have been easier for me to fit the contents into this framework.&lt;/p&gt;

&lt;p&gt;Therefore, I strongly suggest conducting our research and preparing our speech with this framework; then, it will help us draw better results in our research and deliver the main points more clearly to others.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;resting&quot;&gt;Resting&lt;/h3&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-04-16-Dr Zhu Group Meeting Log/Decanting-Guide.png&quot; width=&quot;600&quot; /&gt;   &lt;figcaption&gt;
Figure . Decanting Guide []
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;It is said that decanting is necessary to enjoy wine in it‚Äôs best quality. Likewise, our slides do want to have resting.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;[] L. Mao, ‚ÄúGroup Lasso,‚Äù Lei Mao‚Äôs Log Book. [Online]. Available: https://leimao.github.io/blog/Group-Lasso/. [Accessed: 17-Apr-2021].&lt;/p&gt;

&lt;p&gt;[] ‚ÄúLasso (statistics),‚Äù Wikipedia, 14-Apr-2021. [Online]. Available: https://en.wikipedia.org/wiki/Lasso_(statistics). [Accessed: 17-Apr-2021].&lt;/p&gt;

&lt;p&gt;[] P. P. H. Winston, ‚ÄúHow to Speak,‚Äù MIT OpenCourseWare. [Online]. Available: https://ocw.mit.edu/resources/res-tll-005-how-to-speak-january-iap-2018/. [Accessed: 17-Apr-2021].&lt;/p&gt;

&lt;p&gt;[] E. Golman, ‚ÄúThe Ultimate Guide to Decanting &amp;amp; Aerating Kosher Wine,‚Äù Kosherwine.com, 18-Dec-2020. [Online]. Available: https://www.kosherwine.com/discover/the-ultimate-guide-to-decanting-kosher-wine. [Accessed: 17-Apr-2021].&lt;/p&gt;

&lt;p&gt;[]&lt;/p&gt;
</description>
        <pubDate>Fri, 16 Apr 2021 00:00:00 -0500</pubDate>
      </item>
    
      <item>
        <title>(Research Proj) Spatio-Temporal Graph Convolutional Networks</title>
        <link>/research/ml&dl/2021/04/14/STGCN.html</link>
        <guid isPermaLink="true">/research/ml&dl/2021/04/14/STGCN.html</guid>
        <description>&lt;p&gt;This article is about a review of the paper &lt;a href=&quot;https://arxiv.org/abs/1709.04875&quot;&gt;[1]&lt;/a&gt;, and is the second part of a personal &lt;a href=&quot;#my research proj&quot;&gt;research project&lt;/a&gt; to utilize the use of Graph Neural Network to the field of cascading outage prediction or detection.&lt;/p&gt;

&lt;p&gt;I plan to utilize the model architecture used in this paper for cascading outage prediction.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jhyun0919/GNN-and-Power-Systems/tree/master/Cascading%20Outage&quot;&gt;Research Project‚Äôs Git Repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. INTRODUCTION&lt;/h1&gt;

&lt;h2 id=&quot;11-what-is-the-problem&quot;&gt;1.1. What is the problem?&lt;/h2&gt;

&lt;p&gt;Transportation plays a vital role in everybody‚Äôs daily life. According to a survey in 2015, U.S. drivers spend about 48 minutes on average behind the wheel daily [2]. Under this circumstance, an accurate real-time forecast of traffic conditions is of paramount importance for road users, private sectors, and governments.&lt;/p&gt;

&lt;p&gt;There were many efforts to monitor the current status of traffic conditions and to predict the future. These studies aimed to contribute to widely used transportation services, such as flow control, route planning, and navigation.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;12-history-of-previous-approaches&quot;&gt;1.2. History of previous approaches&lt;/h2&gt;

&lt;p&gt;Previous studies on traffic prediction can be roughly divided into two categories: Dynamical modeling and Data-driven methods.&lt;/p&gt;

&lt;p&gt;Many researchers are shifting their attention to data-driven approaches, since &lt;strong&gt;dynamical modeling&lt;/strong&gt; methods have drawbacks as follow. üò°&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It requires sophisticated systematic programming. &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;It consumes massive computational power. &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Impractical assumptions and simplifications among the modeling degrade the prediction accuracy. &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As a result, &lt;strong&gt;Deep learning approaches&lt;/strong&gt;, which are major representatives of &lt;strong&gt;data-driven approaches&lt;/strong&gt;, have been widely and successfully applied to various traffic tasks. Significant progress has been made in following works. üòÅ&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deep belief network (DBN) [3] &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Stacked auto-encoder (SAE) [4] &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, it is difficult for these dense networks to extract spatial and temporal features from the input jointly. üò°&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;To take full advantage of spatial features, some researchers use &lt;strong&gt;convolutional neural network (CNN)&lt;/strong&gt; to capture adjacent relations among the traffic network, along with employing &lt;strong&gt;recurrent neural network (RNN)&lt;/strong&gt; on time axis. üòÅ&lt;/p&gt;

&lt;p&gt;However, this approach has limitations as follow. üò°&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The normal convolutional operation applied restricts the model to only process grid structures (e.g. images, videos) rather than general domains.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Rrecurrent networks for sequence learning require iterative training, which introduces error accumulation by steps.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;RNN-based networks are widely known to be difficult to train and computationally heavy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;13-proposed-approach&quot;&gt;1.3. Proposed Approach&lt;/h2&gt;

&lt;p&gt;For overcoming the limitations of previous approaches, the author introduces several strategies to effectively model temporal dynamics and spatial dependencies of traffic flow. üòÅ&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To fully utilize spatial information, the author models the traffic network by a general graph instead of treating it separately (e.g. grids or segments).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;To handle the inherent deficiencies of recurrent networks, the author employs a fully convolutional structure on time axis.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Above all, the author proposes a novel deep learning architecture, &lt;strong&gt;the spatio-temporal graph convolutional networks (STGCN)&lt;/strong&gt;, for traffic forecasting tasks. ü§ò
&lt;!-- This architecture comprises several spatio-temporal convolutional blocks, which are a combination of graph convolutional layers [Defferrard et al., 2016] and convolutional sequence learning layers, to model spatial and temporal dependencies. --&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. IS THERE ANY DRAWBACK IN STGCN??? &lt;/span&gt;
&lt;br /&gt;
&lt;span style=&quot;color:blue&quot;&gt; A. &lt;/span&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;2-preliminary&quot;&gt;2. PRELIMINARY&lt;/h1&gt;
&lt;h2 id=&quot;21-traffic-prediction-on-road-graphs&quot;&gt;2.1. Traffic Prediction on Road Graphs&lt;/h2&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;traffic-forecast-as-a-typical-time-series-prediction-problem&quot;&gt;Traffic forecast as a typical time-series prediction problem&lt;/h4&gt;

&lt;p&gt;Predicting the most likely traffic measurements (e.g. speed or traffic flow) in the next $H$ time steps given the previous $M$ traffic observations as,&lt;/p&gt;

\[\hat{v}_{t+1}, ..., \hat{v}_{t+H} = \underset{v_{t+1}, ..., v_{t+H}}{\mathrm{argmax}}\log{P(v_{t+1}, ..., v_{t+H} | v_{t-M+1}, ..., v_{t})}

\tag{1}\]

&lt;p&gt;where $v_t \in \mathbb{R}^{n}$ is an observation vector of $n$ road segments at time step $t$, each element of which records historical observation for a single road segment.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;graph-structured-traffic-time-series-problem&quot;&gt;Graph structured traffic time series problem&lt;/h4&gt;

&lt;p&gt;In this work, the author defines the &lt;strong&gt;traffic network on a graph&lt;/strong&gt; and focuses on &lt;strong&gt;structured traffic time series&lt;/strong&gt;. The traffic network is represented as an undirected graph (or directed one) $\mathcal{G_t}$ with weights $w_{ij}$ as shown in Figure 1.&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-04-14-STGCN/graph_traffic_data.png&quot; width=&quot;400&quot; /&gt;   &lt;figcaption&gt;
Figure 1. Graph-structured traffic data. [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;At the $t$-th time step, in graph $\mathcal{G}_t = (\mathcal{V}_t, \mathcal{E}, W)$,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathcal{V}_t$ is a finite set of vertices, corresponding to the observations from $n$ monitor stations at time $t$.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$\mathcal{E}$ is a set of edges, indicating the connectedness between stations.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$W \in \mathbb{R}^{n \times n}$ denotes the weighted adgacency matrix of $\mathcal{G}_t$.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. DOES EDGES CAN HAVE ITS OWN TIME VARIANT VARIABLES??? &lt;br /&gt; ‚ÄÉ (e.g. temperature of transmission lines in power system) &lt;/span&gt;
&lt;br /&gt;
&lt;span style=&quot;color:blue&quot;&gt; A. &lt;/span&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;22-convolutions-on-graphs&quot;&gt;2.2. Convolutions on Graphs&lt;/h2&gt;

&lt;!-- A standard convolution for regular grids, such as images and video, is clearly not applicable to general graphs. There are two basic approaches currently exploring how to generalize CNNs to structured data forms.

- Expanding the **spatial** definition of a convolution [5]

- Manipulating in the **spectral** domain with graph Fourier transforms [6]

The first approach, using spatial convolution, rearranges the vertices into certain grid forms which can be processed by normal convolutional operations.
The second approach introduces the spectral framework to apply convolutions in spectral domains, often named as the spectral graph convolution.

&lt;span style=&quot;color:green&quot;&gt;
Q. PROS &amp; CONS OF SPATIAL &amp; SPECTRAL???
&lt;/span&gt;

&lt;br&gt;

&lt;span style=&quot;color:green&quot;&gt;
Q. WHY AUTHOR CHOOSE SPECTRAL APPROACHES???
&lt;/span&gt;

&lt;br&gt; --&gt;

&lt;p&gt;In this paper, the author introduces the notion of &lt;strong&gt;graph convolution operator&lt;/strong&gt; ‚Äú$*\mathcal{G}$‚Äù  based on &lt;strong&gt;the conception of spectral filtering&lt;/strong&gt; [7, 8].&lt;/p&gt;

\[\begin{aligned} y &amp;amp; = \Theta *\mathcal{G} x \\&amp;amp; = \Theta (L) x \\&amp;amp; = \Theta (U \Lambda U^T) x \\&amp;amp;= U \Theta (\Lambda) U^T x \end{aligned}
\tag{2}\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$x \in \mathbb{R}^{n}$ is a graph signal as an input (data from each vertex).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$y \in \mathbb{R}^{n}$ is filtered signal as an output.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$\Theta(\cdot)$ is a kernel (characteristic of the filter).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$U = [u_0, ‚Ä¶, u_{n-1}] \in \mathbb{R}^{n\times n}$, Graph Fourier basis, is the matrix of eigenvectors of the normalized graph Laplacian $L$.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$\Lambda =diag([\lambda_0, ‚Ä¶, \lambda_{n-1}]) \in \mathbb{R}^{n\times n}$ is the matrix of eigenvalues of the normalized graph Laplacian $L$.  &lt;br /&gt;&lt;br /&gt;
(check details of Laplacian matrix in &lt;a href=&quot;#appendix 7.1&quot;&gt;appendix 7.1&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a result, a graph signal $x$ is filtered by a kernel $\Theta$ can be defined with multiplication between $\Theta$ and graph Fourier transform $U^T x$ [7].&lt;/p&gt;

&lt;p&gt;(check details of graph Fourier transform in &lt;a href=&quot;#appendix 7.2&quot;&gt;appendix 7.2&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;In addition, due to its matrix multiplication, the computational complexity of graph convolution is $\mathcal{O}(n^2)$.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. WHY ‚ÄúNORMALIZED‚Äù L??? &lt;/span&gt;
&lt;br /&gt;
&lt;span style=&quot;color:blue&quot;&gt; A. &lt;/span&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;!-- &lt;span style=&quot;background-color: #FFFF00&quot;&gt;Marked text&lt;/span&gt; --&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;3-proposed-model&quot;&gt;3. PROPOSED MODEL&lt;/h1&gt;

&lt;h2 id=&quot;31-network-architecture&quot;&gt;3.1. Network Architecture&lt;/h2&gt;

&lt;p&gt;The author proposed a model architecture called spatio-temporal graph convolutional networks (STGCN). As shown in Figure 2, STGCN is composed of several spatio-temporal convolutional blocks.
Each spatio-temporal convolutional blocks is formed as a ‚Äúsandwich‚Äù structure with two gated sequential convolution layers and one spatial graph convolution layer in between.&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-04-14-STGCN/stgcn_architecture.png&quot; width=&quot;666&quot; /&gt;   &lt;figcaption&gt;
Figure 2. Architecture of spatio-temporal graph convolutional networks (STGCN). [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;32-graphs-cnns-for-extracting-spatial-features&quot;&gt;3.2. Graphs CNNs for Extracting Spatial Features&lt;/h2&gt;

&lt;p&gt;The traffic network generally organizes as a graph structure. Since 2-D convolutions on grids can only capture the spatial locality, attributes of traffic networks (graph structured data) were not fully extracted by 2-D convolution filters.&lt;/p&gt;

&lt;p&gt;Accordingly, in STGCN, the graph convolution is employed directly on graph-structured data to extract highly meaningful patterns and features in the space domain.&lt;/p&gt;

&lt;p&gt;However, there are two issues of the graph convolution expressed in Eq. (2). üò°&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The kernel is not localized in space yet.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;The computation of kernel $\Theta$ in graph convolution by Eq. (2) can be expensive due to $\mathcal{O}(n^2)$ multiplications with graph Fourier basis.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The authors applied two approximation strategies to overcome these issues. üòÅ&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;chebyshev-polynomials-approximation&quot;&gt;Chebyshev Polynomials Approximation&lt;/h4&gt;

&lt;p&gt;Chebyshev polynomial $T_k(x)$ is used to approximate kernels as a truncated expansion of order $K-1$ as follows.&lt;/p&gt;

\[\begin{aligned}\Theta (\Lambda) &amp;amp; = \sum_{k=0}^{K-1} \theta_k \Lambda^k \\ &amp;amp;\approx \sum_{k=0}^{K-1} \theta_k T_k(\tilde{\Lambda})\end{aligned}\]

&lt;p&gt;where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\tilde{\Lambda} = 2 \frac{\Lambda}{\lambda_{max}}-I_n$ ($\lambda_{max}$ denotes the largest eigenvalues of $L$).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$\theta \in \mathbb{R}^{n}$ is a vector of polynomial coefficient.  &lt;br /&gt;&lt;br /&gt;
(check details of Chebyshev polynomials approximation in &lt;a href=&quot;#appendix 7.3&quot;&gt;appendix 7.3&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using the Chebyshev polynomials approximation, the graph convolution can then be rewritten as,&lt;/p&gt;

\[\begin{aligned} \Theta*\mathcal{G}x  &amp;amp;= \Theta(L)x \\ &amp;amp;\approx \sum_{k=0}^{K-1} \theta_k T_k(\tilde{L})x \end{aligned}
\tag{3}\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$T_k(\tilde{L})\in\mathbb{R}^{n\times n}$ is the Chebyshev polynomial of order $k$  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$\tilde{L} \in \mathbb{R}^{n \times n}$ the scaled Laplacian $\tilde{L} = 2\frac{L}{\lambda_{max}} - I_n$.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As a result, we can localize the filter by restricting kernel $\Theta$ to a polynomial of order $k$.&lt;/p&gt;

&lt;p&gt;Moreover, by recursively computing $K$ convolutions through the polynomial approximation, the cost of Eq. (2) can be reduced to $\mathcal{O}(K|\mathcal{E}|)$ as Eq. (3) shows [8].&lt;/p&gt;

&lt;p&gt;(check details of localizing convolution filters in &lt;a href=&quot;#appendix 7.4&quot;&gt;appendix 7.4&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. RESCALED??? WHY??? &lt;/span&gt;
&lt;br /&gt;
&lt;span style=&quot;color:blue&quot;&gt; A. It makes eigenvalues in the range [-1, 1] to prepare for the recursive computation. &lt;/span&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. WHY |E| IN COMPUTAIONAL COMPLEXITY??? &lt;/span&gt;
&lt;br /&gt;
&lt;span style=&quot;color:blue&quot;&gt; A. &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;1st-order-approximation&quot;&gt;1st-order Approximation&lt;/h4&gt;

&lt;p&gt;A layer-wise linear formulation can be defined by stacking multiple localized graph convolutional layers with the first-order approximation of graph Laplacian [9].&lt;/p&gt;

&lt;!-- Consequently, a deeper architecture can be constructed to recover spatial information in depth without being limited to the explicit parameterization given by the polynomials. --&gt;

&lt;p&gt;Due to the scaling and normalization in neural networks, we can further assume that $\lambda_{max} \approx 2$.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. THE SCALING AND NORMALIZATION IN NEURAL NETS??? &lt;/span&gt;
&lt;br /&gt;
&lt;span style=&quot;color:blue&quot;&gt; A. &lt;/span&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Thus, the Eq. (3) can be simplified to,&lt;/p&gt;

\[\begin{aligned} \Theta*\mathcal{G}x &amp;amp; \approx \theta_0x + \theta_1(\frac{2}{\lambda_{max}}L-I_n)x \\ &amp;amp;\approx \theta_0x - \theta_1(D^{-1/2}WD^{-1/2})x \end{aligned}
\tag{4}\]

&lt;p&gt;where $\theta_0$, $\theta_1$ are two shared parameters of the kernel.&lt;/p&gt;

&lt;p&gt;In order to constrain parameters and stabilize numerical performances, $\theta_0$ and $\theta_1$ are replaced by a single parameter $\theta$ by letting $\theta = \theta_0 = -\theta_1$.&lt;/p&gt;

&lt;p&gt;Then, the graph convolution can be alternatively expressed as,&lt;/p&gt;

\[\begin{aligned} \Theta*\mathcal{G}x &amp;amp;= \theta(I_n + D^{-1/2}WD^{-1/2})x \\&amp;amp;= \theta(\tilde{D}^{-1/2}\tilde{W}\tilde{D}^{-1/2})x \end{aligned} \tag{5}\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$W$ is renormalized by $\tilde{W} = W + I_n$  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$D$ is renormalized by $\tilde{D_{ii}} = \sum_j{\tilde{W}_{ij}}$.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- Applying a stack of graph convolutions with the 1st-order approximation vertically that achieves the similar effect as $K$-localized convolutions do horizontally, all of which exploit the information from the $(K-1)$-order neighborhood of central nodes. --&gt;
&lt;p&gt;The authors stack the 1st-order approximation $K$ layers vertically. In this scenario, $K$ is the number of successive filtering operations, and the stack achieves the similar effect as $K$-localized convolutions do horizontally.&lt;/p&gt;

&lt;p&gt;Additionally, the layer-wise linear structure is parameter-economic and highly efficient for large-scale graphs, since the order of the approximation is limited to one.&lt;/p&gt;

&lt;p&gt;(check details of localizing convolution filters in &lt;a href=&quot;#appendix 7.4&quot;&gt;appendix 7.4&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;generalization-of-graph-convolutions&quot;&gt;Generalization of Graph Convolutions&lt;/h4&gt;

&lt;p&gt;The graph convolution operator ‚Äú$*\mathcal{G}$‚Äù defined on $x \in \mathbb{R}^n$ can be extended to multi-dimensional tensors.&lt;/p&gt;

&lt;p&gt;For a signal $C_i$ channels $X \in \mathbb{R}^{n \times C_i}$, the graph convolution can be generalized by,&lt;/p&gt;

\[y_i = \sum_{i=1}^{C_i} \Theta_{ij}(L)x_i \in \mathbb{R}^n, 1 \leq j \leq C_o \tag{6}\]

&lt;p&gt;with the $C_i \times C_o$ vectors of Chebyshev coefficients $\Theta_{ij} \in \mathbb{R}^K$ ($C_i$, $C_o$ are the size of input and output of the feature maps, respectively).&lt;/p&gt;

&lt;p&gt;The graph convolution for 2-D variables is denoted as ‚Äú$\Theta *\mathcal{G}X$‚Äù with $\Theta \in \mathbb{R}^{K \times C_i \times C_o}$.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;33-gated-cnns-for-extracting-temporal-features&quot;&gt;3.3. Gated CNNs for Extracting Temporal Features&lt;/h2&gt;

&lt;p&gt;RNNs for traffic prediction still suffer from following. üò°&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;time-consuming iterations  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;complex gate mechanisms  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;slow response to dynamic changes.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On the contrary, CNNs have the superiority as follows. üòÅ&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;fast training  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;simple structures  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;no dependency constraints to previous steps.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The authors employ entire convolutional structures on time axis to capture temporal dynamic behaviors of traffic flows.&lt;/p&gt;

&lt;p&gt;This specific design allows parallel and controllable training procedures through multi-layer convolutional structures formed as hierarchical representations.&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-04-14-STGCN/temporal_gated_conv.png&quot; width=&quot;273&quot; /&gt;   &lt;figcaption&gt;
Figure 3. Temporal Gated-Conv Block. [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;For each node in graph $\mathcal{G}$, the temporal convolution explores $K_t$ neighbors of input elements without padding which leading to shorten the length of sequences by $K_{t}-1$ each time. Thus, input of temporal convolution for each node can be regarded as a length-$M$ sequence with $C_i$ channels as $Y \in \mathbb{R}^{M \times C_i}$.&lt;/p&gt;

&lt;p&gt;The convolution kernel $\Gamma \in \mathbb{R}^{K_t \times C_i \times 2C_o}$ is designed to map the input $Y$ to a single output element $[P : Q] \in \mathbb{R}^{(M-K_t+1) \times (2C_o)}$ ($P, Q$ is split in half with the same size of channels).&lt;/p&gt;

&lt;p&gt;As a result, the temporal gated convolution can be defined as,&lt;/p&gt;

\[\Gamma * \tau Y=P \odot \sigma (Q) \in \mathbb{R}^{(M-K_t+1) \times C_o} \tag{7}\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$P, Q$ are input of gates in GLU respectively  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$\odot$ denotes the element-wise Hadamard product.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$\sigma$ is sigmoid gate controls which input $P$ of the current states are relevant for discovering compositional structure and dynamic variances in time series.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-04-14-STGCN/GLU.png&quot; width=&quot;444&quot; /&gt;   &lt;figcaption&gt;
Figure 4. Structure of Gated Linear Unit (GLU). [12]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. WHY GLU, NOT ReLU OR SOMETHING ELSE??? &lt;/span&gt;
&lt;br /&gt;
&lt;span style=&quot;color:blue&quot;&gt; A. &lt;/span&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The non-linearity gates contribute to the exploiting of the full input filed through stacked temporal layers as well. Furthermore, residual connections are implemented among stacked temporal convolutional layers. Similarly, the temporal convolution can also be generalized to 3-D variables by employing the same convolution kernel $\Gamma$ to every node $\mathcal{Y}_i \in \mathbb{R}^{M \times C_i}$ (e.g. sensor stations) in $\mathcal{G}$ equally, noted as ‚Äú$\Gamma * \tau \mathcal{Y}$‚Äù with $\mathcal{Y} \in \mathbb{R}^{M \times n \times C_i}$.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;34-spatio-temporal-convolutional-block&quot;&gt;3.4. Spatio-Temporal Convolutional Block&lt;/h2&gt;

&lt;p&gt;In order to fuse features from both spatial and temporal domains, the spatio-temporal convolutional block (ST-Conv block) is constructed to jointly process graph-structured time series.&lt;/p&gt;

&lt;!-- The block itself can be stacked or extended based on the scale and complexity of particular cases. --&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-04-14-STGCN/st_conv_block.png&quot; width=&quot;273&quot; /&gt;   &lt;figcaption&gt;
Figure 5. Spatio-Temporal Convolutional (ST-Conv) Block. [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;As illustrated in Figure 5, the spatial layer in the middle is to bridge two temporal layers which can achieve fast spatial-state propagation from graph convolution through temporal convolutions.&lt;/p&gt;

&lt;p&gt;The ‚Äúsandwich‚Äù structure also helps the network sufficiently apply bottleneck strategy to achieve scale compression and feature squeezing by downscaling and upscaling of channels $C$ through the graph convolutional layer.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. WHY IT HAS SANDWICH STRUCTURE??? &lt;/span&gt;
&lt;br /&gt;
&lt;span style=&quot;color:blue&quot;&gt; A. &lt;/span&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Moreover, layer normalization is utilized within every ST-Conv block to prevent overfitting.&lt;/p&gt;

&lt;p&gt;The input and output of ST-Conv blocks are all 3-D tensors. For the input $v^l \in \mathbb{R}^{M \times n \times C^l}$ of block $l$, the output $v^{l+1} \in \mathbb{R}^{(M-2(K_t-1)) \times n \times C^{l+1}}$is computed by,&lt;/p&gt;

\[v^{l+1} = \Gamma_1^l * \tau ReLU(\Theta^l * \mathcal{G}(\Gamma_0^l * \tau v^l)) \tag{8}\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\Gamma_0^l$, $\Gamma_1^l$ are the upper and lower temporal kernel within block $l$, respectively.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$\Theta^l$ is the spectral kernel of graph convolution  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$ReLU(\cdot)$ denotes the rectified linear units function.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-04-14-STGCN/st_convs_fcn.png&quot; width=&quot;273&quot; /&gt;   &lt;figcaption&gt;
Figure 6. The framework STGCN consists of two ST-Conv blocks and a fully-connected output layer in the end. [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;35-loss-function&quot;&gt;3.5. Loss Function&lt;/h2&gt;

&lt;p&gt;We use L2 loss to measure the performance of our model.&lt;/p&gt;

\[L(\hat{v};W_{\theta})=\sum_{t}\| \hat{v}(v_{t-M+1}, ..., v_t, W_\theta) - v_{t+1} \|^2 \tag{9}\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$W_\theta$ are all trainable parameters in the model  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$v_{t+1}$ is the ground truth  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$\hat{v}(\cdot)$ denotes the model‚Äôs prediction  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;36-recap-of-the-main-characteristics-stgcn&quot;&gt;3.6. Recap of the Main Characteristics STGCN&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;STGCN is a universal framework to process &lt;strong&gt;structured time series&lt;/strong&gt;.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;It is not only able to tackle traffic network modeling and prediction issues but also to be applied to more &lt;strong&gt;general spatio-temporal sequence learning tasks&lt;/strong&gt;.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;The spatio-temporal block combines graph convolutions and gated temporal convolutions, which can extract the most useful &lt;strong&gt;spatial features&lt;/strong&gt; and capture the most essential &lt;strong&gt;temporal features&lt;/strong&gt; coherently.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;The model is entirely composed of convolutional structures and therefore achieves &lt;strong&gt;parallelization&lt;/strong&gt; over input with fewer parameters and faster training speed.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;4-experiments--results&quot;&gt;4. EXPERIMENTS &amp;amp; RESULTS&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;41-dataset-description&quot;&gt;4.1. Dataset Description&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;BJER4&lt;/strong&gt; was gathered from the major areas of east ring No.4 routes in &lt;em&gt;Beijing City&lt;/em&gt; by double-loop detectors.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The traffic data are aggregated every 5 minutes.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;There are 12 roads selected for the experiment.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;PeMSD7&lt;/strong&gt; was collected from Caltrans Performance Measurement System (PeMS) in real-time by over 39,000 sensor stations, deployed across the major metropolitan areas of &lt;em&gt;California state&lt;/em&gt; highway system.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The traffic data are aggregated every 5 minutes.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;The authors randomly select a medium and a large scale among the District 7 of California containing 228 and 1,026 stations, labeled as &lt;strong&gt;PeMSD7(M)&lt;/strong&gt; and &lt;strong&gt;PeMSD7(L)&lt;/strong&gt;, respectively, as data sources (shown in the left of Figure 7).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-04-14-STGCN/PeMS_PeMSD7.png&quot; width=&quot;666&quot; /&gt;   &lt;figcaption&gt;
Figure 7. PeMS sensor network in District 7 of California (left), each dot denotes a sensor station; Heat map of weighted adjacency matrix in PeMSD7(M) (right). [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;42-data-preprocessing&quot;&gt;4.2. Data Preprocessing&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Interpolation method: Linear interpolation  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Normalization method: Z-score  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

\[\begin{aligned}Z &amp;amp;= \frac{x-\mu}{\sigma}, \\ &amp;amp; \text{where } \mu \text{ is mean and } \sigma \text{ is standard deviation}.\end{aligned}\]

&lt;!-- - The weighted adjacency matrix $W$

$$
w_{ij} = \begin{cases} exp(-\frac{d_{ij}^{2}}{\sigma^{2}})&amp;, &amp; i\neq j &amp; \text{and} &amp; exp(-\frac{d_{ij}^{2}}{\sigma^{2}}) \geq \epsilon \\ 0 &amp;, &amp; \text{otherwise} \end{cases}
\tag{10}
$$ --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;43-experimental-settings&quot;&gt;4.3. Experimental Settings&lt;/h2&gt;

&lt;p&gt;All the tests use 60 minutes as the historical time window.&lt;/p&gt;

\[\hat{v}_{t+1}, ..., \hat{v}_{t+H} = \underset{v_{t+1}, ..., v_{t+H}}{\mathrm{argmax}}\log{P(v_{t+1}, ..., v_{t+H} | v_{t-M+1}, ..., v_{t})}

\tag{1}\]

&lt;p&gt;With Eq. (1) and the time window setting, 12 observed data points ($M = 12$) are used to forecast traffic conditions in the next 15, 30, and 45 minutes ($H = 3, 6, 9$).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;evaluation-metric&quot;&gt;Evaluation Metric&lt;/h4&gt;

&lt;p&gt;To measure and evaluate the performance of different methods, three following metrics are adopted.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mean Absolute Errors (MAE)&lt;/li&gt;
&lt;/ul&gt;

\[\frac{1}{n}\sum_{1}^{n}{\| y_i - \hat{y_i} \|}\]

&lt;ul&gt;
  &lt;li&gt;Mean Absolute Percentage Errors (MAPE)&lt;/li&gt;
&lt;/ul&gt;

\[\frac{100}{n}\sum_{1}^{n}{ \frac{y_i - \hat{y_i}}{y_i} }\]

&lt;ul&gt;
  &lt;li&gt;Root Mean Squared Errors (RMSE)&lt;/li&gt;
&lt;/ul&gt;

\[\frac{1}{n}\sum_{1}^{n}{\sqrt{(y_i - \hat{y_i})^2}}\]

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. WHY DO WE NEED MULTIPLE METRICS??? &lt;/span&gt;
&lt;br /&gt;
&lt;span style=&quot;color:blue&quot;&gt; A. &lt;/span&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;baselines&quot;&gt;Baselines&lt;/h4&gt;

&lt;p&gt;The authors set up two types of models based on STGCN framework.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;STGCN(Cheb) with the Chebyshev polynomials approximation  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;STGCN(1st) with the 1st-order approximation ($K=1$).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;STGCN based models are compared with following baselines.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Historical Average (HA)  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Linear Support Vector Regression (LSVR)  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Auto-Regressive Integrated Moving Average (ARIMA)  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Feed-Forward Neural Network (FNN)  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Full-Connected LSTM (FC-LSTM)  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Graph Convolutional GRU (GCGRU)  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;44-experimental-results&quot;&gt;4.4. Experimental Results&lt;/h2&gt;

&lt;h4 id=&quot;benefits-of-spatial-topology&quot;&gt;Benefits of Spatial Topology&lt;/h4&gt;

&lt;p&gt;Through effectively utlizing spatial structure of the given data, the STGCN based models have achieved a significant improvement on short and mid-and-long term forecasting.&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-04-14-STGCN/table1.png&quot; width=&quot;600&quot; /&gt;   &lt;figcaption&gt;
Table 1. Performance comparison of different approaches on the dataset BJER4. [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-04-14-STGCN/table2.png&quot; width=&quot;800&quot; /&gt;   &lt;figcaption&gt;
Table 2. Performance comparison of different approaches on the dataset PeMSD7. [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-04-14-STGCN/result1.png&quot; width=&quot;666&quot; /&gt;   &lt;figcaption&gt;
Figure 8. Speed prediction in the morning peak and evening rush hours of the dataset PeMSD7. [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;training-efficiency-and-generalization&quot;&gt;Training Efficiency and Generalization&lt;/h4&gt;

&lt;p&gt;To see the benefits of the convolution along time axis in our proposal, we summarize the comparison of training time between STGCN and GCGRU in Table 3.&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-04-14-STGCN/table3.png&quot; width=&quot;500&quot; /&gt;   &lt;figcaption&gt;
Table 3. Time consumptions of training on the dataset PeMSD7. [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In order to further investigate the performance of compared deep learning models, the authors plot the RMSE and MAE of the test set of PeMSD7(M) during the training process, see Figure 9.&lt;/p&gt;

&lt;p&gt;Those figures also suggest that the models can achieve muchfaster training procedure and easier convergences. Thanks to the special designs in ST-Conv blocks, the models have superior performances in balancing time consumption and parameter settings.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. HOW IT ACHIEVES SUPERIOR PERFORMANCE IN BALANCING TIME COMSUMPTION &amp;amp; PARAMETER SETTING??? &lt;/span&gt;
&lt;br /&gt;
&lt;span style=&quot;color:blue&quot;&gt; A. &lt;/span&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-04-14-STGCN/result2.png&quot; width=&quot;666&quot; /&gt;   &lt;figcaption&gt;
Figure 9. Test RMSE versus the training time (left); Test MAE ver- sus the number of training epochs (right). (PeMSD7(M)) [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Moreover, the number of parameters in STGCN ($4.54 \times 10^5$) only accounts for around two third of GCGRU, and saving over 95% parameters compared to FC-LSTM.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;5-conclusion&quot;&gt;5. CONCLUSION&lt;/h1&gt;

&lt;p&gt;In this paper, the authors propose&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A novel deep learning framework STGCN for traffic prediction.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Integrating graph convolution and gated temporal convolution through spatio-temporal convolutional blocks.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The experiments shows&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Benefits of spatial topology  &lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;The model outperforms other state-of-the-art methods on two real-world datasets.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
      &lt;li&gt;The results indicate its great potentials on exploring spatio-temporal structures from the input.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Training efficiency and generalization  &lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;Faster training (parallelization).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
      &lt;li&gt;Fewer parameters with flexibility and scalability.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. WHY/HOW IT ACHEIVES FELXIBILITY AND SCALABILITY??? &lt;/span&gt;
&lt;br /&gt;
&lt;span style=&quot;color:blue&quot;&gt; A. &lt;/span&gt;
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;These features are quite promising and practical for scholarly development and large-scale industry deployment. In the future, we will further optimize the network structure and parameter settings.&lt;/p&gt;

&lt;p&gt;Moreover, the proposed framework can be applied into more general spatio-temporal structured sequence forecasting scenarios, such as evolving of social networks, and preference prediction in recommendation systems, etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;a name=&quot;my research proj&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;6-my-research-project&quot;&gt;6. MY RESEARCH PROJECT&lt;/h1&gt;

&lt;h2 id=&quot;purpose--goal&quot;&gt;Purpose / Goal&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Forecast or detect&lt;/strong&gt; (in real-time) cascading outages in power systems.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Use a GNN based model to predict or detect cascading outages.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;step-1-data-generation&quot;&gt;Step 1. Data Generation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Need to introduce &lt;strong&gt;randomness&lt;/strong&gt; to create large size of dataset.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Randomness will be given by the method that generated the uncertain loads in &lt;a href=&quot;https://arxiv.org/abs/1902.05607&quot;&gt;[16]&lt;/a&gt;:  &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

\[d := d + \mathcal{N}(\mu=0, \sigma = 0.03 * d)\]

    &lt;ul&gt;
      &lt;li&gt;$d$ is a load.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
      &lt;li&gt;$\mathcal{N}$ is normal distribution with mean zero and standard deviation proportional to the load.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. WHICH FEATURES (e.g. $P_d, Q_d, P_g, Q_g$ or outage event time $t_{event}$) SHOULD BE GIVEN RANDOMNESS? &lt;/span&gt;
&lt;br /&gt;
&lt;span style=&quot;color:blue&quot;&gt; A. &lt;/span&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Codes  &lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/jhyun0919/GNN-and-Power-Systems/blob/master/Cascading%20Outage/codes/gen_dataset_n_2.py&quot;&gt;Generate N-2 Contingency Datasets.py&lt;/a&gt;  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/jhyun0919/GNN-and-Power-Systems/blob/master/Cascading%20Outage/codes/1.%20Generate%20Datasets.ipynb&quot;&gt;Generate Datasets.ipynb&lt;/a&gt;  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/jhyun0919/GNN-and-Power-Systems/blob/master/Cascading%20Outage/codes/2.%20EDA.ipynb&quot;&gt;Exploratory Data Analysis.ipynb&lt;/a&gt;  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;step-2-design-the-gnn-model-architecture&quot;&gt;Step 2. Design the GNN Model Architecture&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The model should able to utilze &lt;strong&gt;spatial&lt;/strong&gt; and &lt;strong&gt;temporal&lt;/strong&gt; feature of the data.  &lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;graph structured&lt;/em&gt; data: spatial feature  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;time series&lt;/em&gt; data: temporal feature  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Models based on &lt;strong&gt;STGCN architecture&lt;/strong&gt; [1] can extract and learn the both features (spatial &amp;amp; temporal) from the data.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Codes  &lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;yet not ready‚Ä¶üò≠  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;step-3-train--test-the-model&quot;&gt;Step 3. Train &amp;amp; Test the Model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;ü§î&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;step-4-tune-the-hyper-parameters&quot;&gt;Step 4. Tune the Hyper-parameters&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;ü§î&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;results-what-we-want-to-see&quot;&gt;Results (What we want to see‚Ä¶)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;High enough prediction (or detection) accuracy.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Extremely high computational efficiency compared to old methods.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;A GNN based model having significantly fewer nodes (trainable parameters) than other deep learning models.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;7-appendix&quot;&gt;7. APPENDIX&lt;/h1&gt;

&lt;p&gt;&lt;a name=&quot;appendix 7.1&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;71-laplacian--symmetric-normalized-laplacian-10&quot;&gt;7.1. Laplacian &amp;amp; Symmetric normalized Laplacian [10]&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;laplacian&quot;&gt;Laplacian&lt;/h4&gt;

&lt;p&gt;Given a simple graph $\mathcal{G}$ with $n$ vertices, its Laplacian matrix $L_{n \times n}$ is defined as,&lt;/p&gt;

\[L = D - A\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$D$ is the degree matrix of the graph.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$A$ is adjacency matrix.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The elements of $L$ are given by&lt;/p&gt;

\[L_{ij}:= \begin{cases} deg(v_i) &amp;amp;\text{if } i=j \\ -1 &amp;amp;\text{if } i\neq j \text{ and } v_i \text{ is adjacent to } v_j \\ 0 &amp;amp;\text{otherwise} \end{cases}\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$deg(v_i)$ is the degree of the vertex $i$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;symmetric-normalized-laplacian&quot;&gt;Symmetric normalized Laplacian&lt;/h4&gt;

&lt;p&gt;The symmetric normalized Laplacian matrix is defined as,&lt;/p&gt;

\[L = I - D^{-1/2} A D^{-1/2}\]

&lt;p&gt;The elements of $L^{sym}$ are given by&lt;/p&gt;

\[L_{ij}:=
\begin{cases}
1 &amp;amp;\text{if } i=j \text{ and } deg(v_i) \neq 0\\
-\frac{1}{\sqrt{deg(v_i)deg(v_j)}} &amp;amp;\text{if } i\neq j \text{ and } v_i \text{ is adjacent to } v_j \\
0 &amp;amp;\text{otherwise}
\end{cases}\]

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-04-14-STGCN/laplacian_matrix.png&quot; width=&quot;666&quot; /&gt;   &lt;figcaption&gt;
Figure 10. Example of a General Graph and its Degree, Adjacency, and Laplacian Matrix. [10]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;appendix 7.2&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;72-graph-fourier-transform&quot;&gt;7.2. Graph Fourier transform&lt;/h2&gt;

&lt;p&gt;Graph Fourier transform is a mathematical transform which &lt;em&gt;eigendecomposes&lt;/em&gt; the Laplacian matrix of a graph into eigenvalues and eigenvectors. Analogously to classical Fourier Transform, the &lt;em&gt;eigenvalues&lt;/em&gt; represent &lt;em&gt;frequencies&lt;/em&gt; and &lt;em&gt;eigenvectors&lt;/em&gt; form what is known as &lt;em&gt;a graph Fourier basis&lt;/em&gt; [11].&lt;/p&gt;

&lt;p&gt;Lets assume we have a graph $\mathcal{G}_t = (\mathcal{V}, \mathcal{E}, W)$, where $\mathcal{V}$ is a finite set of $|\mathcal{V}|=n$ verices, $\mathcal{E}$ is a set of edges, and $W \in \mathbb{R}^{n\times n}$ is a weighted adjacency matrix.&lt;/p&gt;

&lt;p&gt;The definition of Laplacian matrix is&lt;/p&gt;

\[L = I_n - D^{-\frac{1}{2}} W D^{-\frac{1}{2}} = U \Lambda U^T \in \mathbb{R}^{n\times n}\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$I_n$ is an identity matrix.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$D \in \mathbb{R}^{n\times n}$ is the diagonal degree matrix with $D_{ii} = \sum_{j}{W_{ij}}$.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$U = [u_0, ‚Ä¶, u_{n-1}] \in \mathbb{R}^{n\times n}$ is a Graph Fourier basis.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$\Lambda =diag([\lambda_0, ‚Ä¶, \lambda_{n-1}]) \in \mathbb{R}^{n\times n}$ is the diagonal matrix of eigenvalues of $L$.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;strong&gt;graph Fourier transform&lt;/strong&gt; of a signal $x \in \mathbb{R}^{n}$ is then deinfed as as $\hat{x} = U^{T}x \in \mathbb{R}^{n}$, and its inverse as $x=U\hat{x}$ [13].  &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;appendix 7.3&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;73-chebyshev-polynomial--approximation-14-15&quot;&gt;7.3. Chebyshev Polynomial &amp;amp; Approximation [14, 15]&lt;/h2&gt;

&lt;p&gt;The Chebyshev polynomials are two sequences of polynomials related to the sine and cosine functions, notated as $T_n(x)$ and $U_n(x)$.&lt;/p&gt;

\[T_n(cos(\theta))=cos(n\theta)\]

\[U_n(cos(\theta))sin(\theta)=sin((n+1)\theta)\]

&lt;p&gt;The Chebyshev polynomials of the first kind are obtained from the recurrence relation as follows.&lt;/p&gt;

\[T_0(x)=1 \\
T_1(x)=x \\
T_{n+1}(x)=2xT_n(x)-T_{n-1}(x)\]

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-04-14-STGCN/Chebyshev_Polynomials_of_the_First_Kind.png&quot; width=&quot;600&quot; /&gt;   &lt;figcaption&gt;
Figure 11. Plot of the first five Tn Chebyshev polynomials of the first kind. [14]
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;One can obtain polynomials very close to the optimal one by expanding the given function in terms of Chebyshev polynomials and then cutting off the expansion at the desired degree.&lt;/p&gt;

\[f(x) \approx \sum_{i=0}^{\infty}{c_i T_{i}(x)}\]

&lt;p&gt;This is similar to the Fourier analysis of the function, using the Chebyshev polynomials instead of the usual trigonometric functions.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;appendix 7.4&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;74-localizing-convolutional-filter-in-space&quot;&gt;7.4. Localizing Convolutional Filter in Space&lt;/h2&gt;

&lt;p&gt;The Eq. (2) represents spectral filtering of graph signal.&lt;/p&gt;

\[\begin{aligned} y &amp;amp; = \Theta *\mathcal{G} x \\&amp;amp; = \Theta (L) x \\&amp;amp; = \Theta (U \Lambda U^T) x \\&amp;amp;= U \Theta (\Lambda) U^T x \end{aligned}
\tag{2}\]

&lt;p&gt;The filter is characterized by its kernel ($\Theta(\cdot)$), and the simplest kernerl we can imagine is a non-parametic filter, i.e. the kernel parameters $\theta_k \in \mathbb{R}$  $(k=1, ‚Ä¶, n)$ are all free.&lt;/p&gt;

\[\Theta(\Lambda) = diag(\theta)\]

&lt;p&gt;In this case, the filter has a limitaion that it is not localized in space. üò°&lt;/p&gt;

&lt;p&gt;However, this issue can be solved by following two strategies. üòÅ&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;a-restricting-kernel-to-polynomial-by-chebyshev-approximation-horizontal-ver&quot;&gt;A. Restricting kernel to polynomial by Chebyshev approximation (Horizontal ver.)&lt;/h4&gt;

&lt;p&gt;We can localize the filter in space by restricting the kernel to a polynomial [8]. As a result, the convolutional filter is restricted as a polynomial filter. Moreover, by using Chebyshev approximation, we could limit the radius of the convolutional operation.&lt;/p&gt;

&lt;p&gt;The localized filter can be expressed as follows.&lt;/p&gt;

\[\begin{aligned}\Theta(\Lambda) &amp;amp;= \sum_{k=0}^{K-1}{\theta_k \Lambda^{k}} \\ &amp;amp;\approx \sum_{k=0}^{K-1}{\theta_k T_{k}(\tilde{\Lambda})}\end{aligned}\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\tilde{\Lambda} = 2 \frac{\Lambda}{\lambda_{max}}-I_n$ ($\lambda_{max}$ denotes the largest eigenvalues of $L$).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$\theta \in \mathbb{R}^{n}$ is a vector of polynomial coefficient.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$K$ is a radius of the convolutional filter.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;b-applying-a-stack-of-1st-order-approximated-kernel-vertical-ver&quot;&gt;B. Applying a stack of 1st-order approximated kernel (Vertical ver.)&lt;/h4&gt;

&lt;p&gt;By 1st-order approximation, we can get a simplified filter as follows.&lt;/p&gt;

\[\Theta(\Lambda) \approx \theta_0 + \theta_1(\frac{2}{\lambda_{max}}\Lambda-I_n)\]

&lt;p&gt;where $\theta_0$, $\theta_1$ are two shared parameters of the kernel.&lt;/p&gt;

&lt;p&gt;Applying a stack of graph convolutions with the 1st-order approximation vertically that achieves the similar effect as $K$-localized convolutions do horizontally, all of which exploit the information from the $(K-1)$-order neighborhood of central nodes [1].&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;p&gt;[1] B. Yu, H. Yin, and Z. Zhu, ‚ÄúSpatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting,‚Äù arXiv.org, 12-Jul-2018.&lt;/p&gt;

&lt;p&gt;[2] B. Tefft, S. Rosenbloom, R. Santos, and T. Triplett, ‚ÄúAmerican Driving Survey: 2014 ‚Äì 2015,‚Äù AAA Foundation, 14-Jun-2018. [Online]. Available: https://aaafoundation.org/american-driving-survey-2014-2015/. [Accessed: 15-Feb-2021].&lt;/p&gt;

&lt;p&gt;[3] Yuhan Jia, Jianping Wu, and Yiman Du, ‚ÄúTraffic speed prediction using deep learning method,‚Äù 2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC), 2016.&lt;/p&gt;

&lt;p&gt;[4] Y. Lv, Y. Duan, W. Kang, Z. Li and F. Wang, ‚ÄúTraffic Flow Prediction With Big Data: A Deep Learning Approach,‚Äù in IEEE Transactions on Intelligent Transportation Systems, vol. 16, no. 2, pp. 865-873, April 2015, doi: 10.1109/TITS.2014.2345663.&lt;/p&gt;

&lt;p&gt;[5] M. Niepert, M. Ahmed, and K. Kutzkov, ‚ÄúLearning Convolutional Neural Networks for Graphs,‚Äù arXiv.org, 08-Jun-2016. [Online]. Available: https://arxiv.org/abs/1605.05273. [Accessed: 05-Apr-2021].&lt;/p&gt;

&lt;p&gt;[6] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, ‚ÄúSpectral Networks and Locally Connected Networks on Graphs,‚Äù arXiv.org, 21-May-2014. [Online]. Available: https://arxiv.org/abs/1312.6203. [Accessed: 05-Apr-2021].&lt;/p&gt;

&lt;p&gt;[7] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst, ‚ÄúThe emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains,‚Äù IEEE Signal Processing Magazine, vol. 30, no. 3, pp. 83‚Äì98, 2013.&lt;/p&gt;

&lt;p&gt;[8] M. Defferrard, X. Bresson, and P. Vandergheynst, ‚ÄúConvolutional Neural Networks on Graphs with Fast Localized Spectral Filtering,‚Äù arXiv.org, 05-Feb-2017. [Online]. Available: https://arxiv.org/abs/1606.09375. [Accessed: 05-Apr-2021].&lt;/p&gt;

&lt;p&gt;[9] T. N. Kipf and M. Welling, ‚ÄúSemi-Supervised Classification with Graph Convolutional Networks,‚Äù arXiv.org, 22-Feb-2017. [Online]. Available: https://arxiv.org/abs/1609.02907v4. [Accessed: 05-Apr-2021].&lt;/p&gt;

&lt;p&gt;[10] ‚ÄúLaplacian matrix,‚Äù Wikipedia, 10-Jan-2021. [Online]. Available: https://en.wikipedia.org/wiki/Laplacian_matrix. [Accessed: 03-Feb-2021].&lt;/p&gt;

&lt;p&gt;[11] ‚ÄúGraph Fourier Transform,‚Äù Wikipedia, 30-Dec-2020. [Online]. Available: https://en.wikipedia.org/wiki/Graph_Fourier_Transform. [Accessed: 01-Mar-2021].&lt;/p&gt;

&lt;p&gt;[12] M. Elbayad, ‚ÄúRethinking the Design of Sequence-to-Sequence Models for Efficient Machine Translation,‚Äù TEL, 03-Nov-2020. [Online]. Available: https://tel.archives-ouvertes.fr/tel-02986998. [Accessed: 09-Apr-2021].&lt;/p&gt;

&lt;p&gt;[13] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst, ‚ÄúThe emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains,‚Äù IEEE Signal Processing Magazine, vol. 30, no. 3, pp. 83‚Äì98, 2013.&lt;/p&gt;

&lt;p&gt;[14] ‚ÄúChebyshev polynomials,‚Äù Wikipedia, 08-Apr-2021. [Online]. Available: https://en.wikipedia.org/wiki/Chebyshev_polynomials. [Accessed: 14-Apr-2021].&lt;/p&gt;

&lt;p&gt;[15] ‚ÄúApproximation theory,‚Äù 11-Jan-2021. [Online]. Available: https://en.wikipedia.org/wiki/Approximation_theory. [Accessed: 14-Apr-2021].&lt;/p&gt;

&lt;p&gt;[16] D. Deka and S. Misra, ‚ÄúLearning for DC-OPF: Classifying active sets using neural nets,‚Äù 2019 IEEE Milan PowerTech, Milan, Italy, 2019, pp. 1-6.&lt;/p&gt;
</description>
        <pubDate>Wed, 14 Apr 2021 00:00:00 -0500</pubDate>
      </item>
    
      <item>
        <title>Householder QR Factorization</title>
        <link>/research/mathematics/2021/01/14/Householder.html</link>
        <guid isPermaLink="true">/research/mathematics/2021/01/14/Householder.html</guid>
        <description>&lt;p&gt;This article was written by referring to the &lt;em&gt;Fall 2020 Numerical Analysis: Linear Algebra Course&lt;/em&gt; and the following links.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.utexas.edu/users/flame/laff/alaff/chapter03-gram-schmidt.html&quot;&gt;ALAFF: Gram-Schmidt Orthogonalization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ULAFF/ALAFF&quot;&gt;ALAFF Git Repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;unitary-matrix&quot;&gt;Unitary Matrix&lt;/h1&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-04-Householder/reflector.png&quot; width=&quot;70%&quot; /&gt;   &lt;figcaption&gt;
Figure 1.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;!-- ###################################################################### --&gt;

&lt;h1 id=&quot;householder-transformation&quot;&gt;Householder transformation&lt;/h1&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-04-Householder/Housev.png&quot; width=&quot;70%&quot; /&gt;   &lt;figcaption&gt;
Figure 2.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;  &lt;script src=&quot;https://gist.github.com/jhyun0919/d8e7ad29b9e0de1bc78e7b4b70bd9b2f.js&quot;&gt;&lt;/script&gt;
 &lt;em&gt;Code 1. Householder transformation in python.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;!-- ###################################################################### --&gt;

&lt;h1 id=&quot;householder-qr-factorization-algorithm&quot;&gt;Householder QR factorization algorithm&lt;/h1&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-04-Householder/HQR_how.png&quot; width=&quot;70%&quot; /&gt;   &lt;figcaption&gt;
Figure 3.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-04-Householder/HQR.png&quot; width=&quot;70%&quot; /&gt;   &lt;figcaption&gt;
Figure 4.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;  &lt;script src=&quot;https://gist.github.com/jhyun0919/c65f24fb8416e3f50c8319d9da21f812.js&quot;&gt;&lt;/script&gt;
 &lt;em&gt;Code 2. Householder QR factorization in python.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;  &lt;script src=&quot;https://gist.github.com/jhyun0919/867a0d120db1ebe0e4ec0a19201f511a.js&quot;&gt;&lt;/script&gt;
 &lt;em&gt;Code 3. Householder LQ factorization in python.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;!-- ###################################################################### --&gt;

&lt;h1 id=&quot;forming-q&quot;&gt;Forming Q&lt;/h1&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-04-Householder/FormQ_how.png&quot; width=&quot;70%&quot; /&gt;   &lt;figcaption&gt;
Figure 5.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-04-Householder/FormQ.png&quot; width=&quot;70%&quot; /&gt;   &lt;figcaption&gt;
Figure 6.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;  &lt;script src=&quot;https://gist.github.com/jhyun0919/9e626b4fa8f7066550540ab721b1114e.js&quot;&gt;&lt;/script&gt;
 &lt;em&gt;Code 4. Form Q from Householder QR factorization in python.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;!-- ###################################################################### --&gt;

&lt;h1 id=&quot;applying-qh&quot;&gt;Applying QH&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;!-- ###################################################################### --&gt;

&lt;h1 id=&quot;orthogonality-of-resulting-q&quot;&gt;Orthogonality of resulting Q&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jhyun0919/ALAFF-in-python/blob/main/Assignments/week03/test_orthogonality.ipynb&quot;&gt;test_orthogonality.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- ###################################################################### --&gt;
&lt;h1 id=&quot;blocked-householder-qr-factorization&quot;&gt;Blocked Householder QR factorization&lt;/h1&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-04-Householder/HQR_blk.png&quot; width=&quot;70%&quot; /&gt;   &lt;figcaption&gt;
Figure 7.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2&gt;&lt;br /&gt;&lt;/h2&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;[1]&lt;/p&gt;
</description>
        <pubDate>Thu, 14 Jan 2021 00:00:00 -0600</pubDate>
      </item>
    
      <item>
        <title>(Python-Dev Tips) PEP8</title>
        <link>/research/cs/2021/01/10/pep8.html</link>
        <guid isPermaLink="true">/research/cs/2021/01/10/pep8.html</guid>
        <description>&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://jhyun0919.github.io/research/cs/2020/10/01/git-tips.html&quot;&gt;Git Basic&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jhyun0919.github.io/research/cs/2020/10/02/virtualenv.html&quot;&gt;Virtual Environment&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;PEP8&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jhyun0919.github.io/research/cs/2020/10/04/jupyter.html&quot;&gt;Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;This article covers what is PEP 8 and how to follow this convention with minimal effort and no difficult. This posting referred to the following articles.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.python.org/dev/peps/pep-0008/&quot;&gt;PEP 8 ‚Äì Style Guide for Python Code&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://realpython.com/python-pep8/&quot;&gt;How to Write Beautiful Python Code With PEP 8&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.freecodecamp.org/news/auto-format-your-python-code-with-black/&quot;&gt;How to Auto-Format Your Python Code with Black&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;what-is-pep-8&quot;&gt;What is PEP 8&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;why-we-need-pep-8&quot;&gt;Why we need PEP 8&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Guidelines for writing Python.&lt;/li&gt;
  &lt;li&gt;Good Readability&lt;/li&gt;
  &lt;li&gt;Revisiting old project&lt;/li&gt;
  &lt;li&gt;Easy to spot bugs&lt;/li&gt;
  &lt;li&gt;Collaboration&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;auto-formatter&quot;&gt;Auto-formatter&lt;/h1&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2020-10-03-pep8/programmer_meme.jpg&quot; width=&quot;80%&quot; /&gt;   &lt;figcaption&gt;
&quot;Any Questions?&quot;
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

</description>
        <pubDate>Sun, 10 Jan 2021 00:00:00 -0600</pubDate>
      </item>
    
      <item>
        <title>(Research Proj) Cascading Outage Simulator - Dynamic</title>
        <link>/research/energy/2021/01/05/Cascading-Outage-Sim-Dynamic.html</link>
        <guid isPermaLink="true">/research/energy/2021/01/05/Cascading-Outage-Sim-Dynamic.html</guid>
        <description>&lt;p&gt;This article is about a review of the paper &lt;a href=&quot;https://ieeexplore.ieee.org/document/7127056&quot;&gt;[1]&lt;/a&gt;, and is the first part of a personal research project to apply Graph Neural Network (GNN) to the field of cascading outage prediction or detection.&lt;/p&gt;

&lt;p&gt;The primary purpose is to create large-scale datasets for training GNN-based models through the cascading outage simulator presented in this paper.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jhyun0919/GNN-and-Power-Systems/tree/master/Cascading%20Outage&quot;&gt;Research Project‚Äôs Git Repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- - [Paper Review in GoodNotes](https://goodnotes.com/shares/#aHR0cHM6Ly93d3cuaWNsb3VkLmNvbS9zaGFyZS8wMkdiT3EyOTc0TERRd0NUdWtmeVF3MzBnI0R5bmFtaWNfTW9kZWxpbmdfb2ZfQ2FzY2FkaW5nX0ZhaWx1cmVfaW5fUG93ZXJfU3lzdGVtcw== ) --&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;1-cascading-outage-simulator&quot;&gt;1. CASCADING OUTAGE SIMULATOR&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what--why&quot;&gt;WHAT &amp;amp; WHY&lt;/h2&gt;

&lt;p&gt;The importance of studying cascading outages has been recognized [2]-[4]. However, since electrical power networks are very large and complex systems [5], understanding the many mechanisms by which cascading outages propagate is challenging [1].&lt;/p&gt;

&lt;p&gt;Cascading outage simulators are designed to set and reproduce scenarios that can occur in the electric grid, and it allow us to study a wide variety of different mechanisms of cascading outages.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;in-the-paper&quot;&gt;IN THE PAPER&lt;/h2&gt;

&lt;p&gt;The paper presents the design of and results from a new non-linear dynamic model of cascading failure in power system,  called &lt;strong&gt;‚ÄúCascading Outage Simulator with Multi-process Integration Capabilities‚Äù (COSMIC)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In COSMIC [1], ‚Ä¶&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Dynamic components&lt;/strong&gt; are modeled using &lt;strong&gt;differential equations&lt;/strong&gt;.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Power flows&lt;/strong&gt; are represented using &lt;strong&gt;non-linear power flow equations&lt;/strong&gt;.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Discrete changes&lt;/strong&gt; (e.g., components failures, load shedding) are described by &lt;strong&gt;a set of equations (constraints)&lt;/strong&gt;.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;COSMIC has the following benefits [1].&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It provids an &lt;strong&gt;open platform&lt;/strong&gt; for research and development.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;dynamic/adaptive time step&lt;/strong&gt; and &lt;strong&gt;recursive islanded time horizons&lt;/strong&gt; implemented in this simulator which allows for faster computations during, near, steady-state regimes, and fine resolution during transient phases.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;It can be easily integrated with High Performance Computing (HPC) clusters to &lt;strong&gt;run many simulations simultaneously&lt;/strong&gt; at a much lower cost.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;2-hybrid-system-modeling-in-cosmic&quot;&gt;2. HYBRID SYSTEM MODELING IN COSMIC&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-hybrid-differential-algebraic-formulation&quot;&gt;A. Hybrid differential-algebraic formulation&lt;/h2&gt;

&lt;p&gt;Dynamic power networks are modeled as sets of DAEs. Each DAE is composed of three parts:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  i. A set of Differential equations
  ii. A set of Algebraic equations
  iii. A set of Constraints
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;i-a-set-of-differential-equations&quot;&gt;i. A set of Differential equations&lt;/h3&gt;

\[\frac{d\mathbf{x}}{dt}=\mathbf{f}(t, \mathbf{x}(t), \mathbf{y}(t), \mathbf{z}(t)) \tag{1}\]

&lt;ul&gt;
  &lt;li&gt;$\mathbf{x}$ is a vector of &lt;strong&gt;continuous state variables&lt;/strong&gt; that change with time according to a set of &lt;strong&gt;differential equations&lt;/strong&gt;.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;(1) represent the &lt;strong&gt;machine dynamics&lt;/strong&gt; (&lt;em&gt;APPENDIX - A&lt;/em&gt;).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;ii-a-set-of-algebraic-equations&quot;&gt;ii. A set of Algebraic equations&lt;/h3&gt;

\[\mathbf{g}(t, \mathbf{x}(t), \mathbf{y}(t), \mathbf{z}(t)) =0 \tag{2}\]

&lt;ul&gt;
  &lt;li&gt;$\mathbf{y}$ is a vector of &lt;strong&gt;continuous state variables&lt;/strong&gt; that have pure &lt;strong&gt;algebraic relationships&lt;/strong&gt; to other variables in the system.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;(2) encapsulate the standard &lt;strong&gt;ac power flow equations&lt;/strong&gt; (&lt;em&gt;APPENDIX - B&lt;/em&gt;).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;(2) is largely dependent on the load models (Figure 1. &lt;a href=&quot;https://goodnotes.com/shares/#aHR0cHM6Ly93d3cuaWNsb3VkLmNvbS9zaGFyZS8wSXR2MFFaQkxqVHNDOERzS3dHYk94RVZ3I0xlY3R1cmVfbm90ZV9jaF8wNg==&quot;&gt;ZIP&lt;/a&gt;E models).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt;Q. WHAT IS EXPONENTIAL MODEL???&lt;/span&gt;&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-05-COSMIC/COSMIC_load_type.png&quot; width=&quot;666&quot; /&gt;   &lt;figcaption&gt;
Figure 1. Illustrates a dramatic impact of load models on algebraic convergence. [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;iii-a-set-of-constarints&quot;&gt;iii. A set of Constarints&lt;/h3&gt;

\[\mathbf{h}(t, \mathbf{x}(t), \mathbf{y}(t), \mathbf{z}(t))&amp;lt;0 \tag{3}\]

&lt;ul&gt;
  &lt;li&gt;$\mathbf{z}$ is a vector of &lt;strong&gt;state variables&lt;/strong&gt; (relay status) that can only take &lt;strong&gt;integer states&lt;/strong&gt; $( z_i ‚àà [0, 1])$.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;(3) represent the &lt;strong&gt;constraints&lt;/strong&gt;.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;If a constraint $\mathbf{h_i}(‚Ä¶)&amp;lt;0$ fails (outage occurs), an associated &lt;strong&gt;counter function&lt;/strong&gt; $\mathbf{d_i}$ (relay) activates.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;cf-what-happens-during-cascading-failures&quot;&gt;cf. What happens during cascading failures?&lt;/h3&gt;

&lt;p&gt;During cascading failures, power systems many undergo &lt;strong&gt;discrete changes&lt;/strong&gt;. The discrete event(s) will consequently &lt;strong&gt;change the systems dynamic response and algebraic equations&lt;/strong&gt;, which may result in cascading failures, system islanding, and large blackouts.&lt;/p&gt;

&lt;p&gt;&lt;!-- - exogenous events (e.g., manual operations, weather) --&gt;&lt;/p&gt;

&lt;p&gt;&lt;!-- - endogenous events (e.g., automatic protective relay actions) --&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;b-relay-modeling&quot;&gt;B. Relay modeling&lt;/h2&gt;

&lt;!-- Major disturbances cause system oscillations as the system seeks a new equilibrium. These oscillations may naturally die out due to the interactions of system inertia, damping, and exciter and governor controls. In order to ensure that relays do not trip due to brief transient state changes, time-delays are added to each protective relay in COSMIC. --&gt;

&lt;p&gt;Major disturbances cause system oscillations, and these oscillations may naturally die out as the system adjusts to a new equilibrium. In order to ensure that relays do not trip due to brief transient state changes, time-delays are added to each protective relay in COSMIC.&lt;/p&gt;

&lt;p&gt;Five types of protective relays are modeled in COSMIC:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Over-current (OC) relays  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Distance (DIST) relays  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Temperature (TEMP) relays  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Under-voltage load shedding (UVLS) relays  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Under-frequency load shedding (UFLS)  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;c-solving-the-hybrid-dae&quot;&gt;C. Solving the hybrid DAE&lt;/h2&gt;

&lt;p&gt;COSMIC used the following two strategies to solve the hybrid DAE.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;COSMIC uses the &lt;strong&gt;trapezoidal rule&lt;/strong&gt; to simultaneously integrate and solve the differential and algebraic equations.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;COSMIC implements a &lt;strong&gt;variable time-step size&lt;/strong&gt; in order to trade-off between the diverse time-scales of the dynamics.  &lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;During transition periods: small step size ‚Üí &lt;strong&gt;fine resolution&lt;/strong&gt;.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
      &lt;li&gt;During steady-state periods: large step size ‚Üí &lt;strong&gt;faster computation&lt;/strong&gt;.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;trapezoidal-rule&quot;&gt;Trapezoidal Rule&lt;/h3&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-05-COSMIC/Trapezoidal_Rule.png&quot; width=&quot;600&quot; /&gt;   &lt;figcaption&gt;
Figure 2. Trapezoidal Rule [6]
  &lt;/figcaption&gt;
&lt;/figure&gt;

\[\int_{a}^{b} f(x) \,dx = \frac{\Delta x}{2}[f(x_0) + 2f(x_1) +... + 2f(x_{n-1}) + f(x_n)]\]

&lt;!-- &lt;p style=&quot;text-align: center;&quot;&gt; ‚Üì &lt;/p&gt;


$$
\mathbf{x}_+ = \mathbf{x} + \frac{\Delta t}{2} [\mathbf{f}(t)+\mathbf{f}(t_+, \mathbf{x}_+, \mathbf{y}_+, \mathbf{z})]
$$

$$
0 = \mathbf{g}(t_+, \mathbf{x}_+, \mathbf{y}_+, \mathbf{z})
$$

- $\mathbf{x}_+=\mathbf{x}(t+\Delta t)$
- $\mathbf{y}_+=\mathbf{y}(t+\Delta t)$ --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;dae-during-discrete-event&quot;&gt;DAE during discrete event&lt;/h3&gt;

\[0 = \mathbf{x} + \frac{t_d-t}{2} [\mathbf{f}(t)+\mathbf{f}(t_d, \mathbf{x}_d, \mathbf{y}_d, \mathbf{z}_d)] \tag{4}\]

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. HOW TO APPLY TRAPEZOIDAL RULE TO A DISCRETE POINT??? &lt;/span&gt;&lt;/p&gt;

\[0=\mathbf{g}(t_d, \mathbf{x}_+, \mathbf{y}_+, \mathbf{z}) \tag{5}\]

\[0&amp;gt;\mathbf{h}(t_d, \mathbf{x}_+, \mathbf{y}_+) \tag{6}\]

\[0=\mathbf{d}(t_d, \mathbf{x}_+, \mathbf{y}_+) \tag{7}\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$t$ is the previous time point.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$t_d$ is the point a discrete event occurs.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Because of the adaptive time step size, COSMIC retains $t_d$ from $t_d = t + \Delta t_d$, in which $\Delta t_d$ is found by &lt;strong&gt;linear interpolation&lt;/strong&gt; of two time steps.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. HOW THE LINEAR INTERPOLATION WORKS IN HERE??? &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;time-domain-simulation-algorithm&quot;&gt;Time-Domain Simulation Algorithm&lt;/h3&gt;

&lt;p&gt;The description of Time-Domain Simulation Algorithm implemented in COSMIC and a corresponding flowchart are as follows.&lt;/p&gt;

&lt;p&gt;The feature of this algorithm that we need to focus is that when network separation occurs, each subnetwork is computed &lt;strong&gt;independently&lt;/strong&gt; in a &lt;strong&gt;recursive&lt;/strong&gt; manner.&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-05-COSMIC/Algorithm.png&quot; width=&quot;555&quot; /&gt;   &lt;figcaption&gt;
Figure 3. Time-Domain Simulation Algorithm [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-05-COSMIC/COSMIC_Flowchart.png&quot; width=&quot;900&quot; /&gt;   &lt;figcaption&gt;
Figure 4. Flowchart of Time-Domain Simulation Algorithm
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;3-experiments-and-results&quot;&gt;3. EXPERIMENTS AND RESULTS&lt;/h1&gt;

&lt;p&gt;In this paper, the performance and characteristics of COSMIC were explored through several experiments as follows.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;a-polar-formulation-vs-rectangular-formulation-in-computational-efficiency&quot;&gt;A. Polar formulation vs. Rectangular formulation in computational efficiency&lt;/h2&gt;

&lt;h3 id=&quot;purpose&quot;&gt;Purpose&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;To compare the computational efficiency of the polar and rectangular formulations of the model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. WHY DO WE NEED TO COMPARE THE PERFOMANCE BETWEEN RECT AND POLAR??? &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;used-test-systems&quot;&gt;Used test systems&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;39-bus system  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;2383-bus system  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;results&quot;&gt;Results&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;On Table I, in 39-bus case, the rectangular formulation required fewer linear solves for different demand losses.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;On Table II, in 2383-bus case, there was no significant improvement for the rectangular formulation over the polar formulation, and the number of linear solves that resulted from both forms were almost identical.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-05-COSMIC/rect_polar_performance.png&quot; width=&quot;600&quot; /&gt;   &lt;figcaption&gt;
Table I &amp;amp; Table II. Performance comparison between Rect &amp;amp; Polar [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. WHY DO WE NEED TO KNOW ABOUT THE DENSITY OF THE JACOBIAN MATRICES??? &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;b-relay-event-illustration&quot;&gt;B. Relay event illustration&lt;/h2&gt;

&lt;h3 id=&quot;purpose-1&quot;&gt;Purpose&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;To illustrate the different relay functions implemented in COSMIC and their time delay algorithms.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;used-test-systems-1&quot;&gt;Used test systems&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;9-bus system&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;results-1&quot;&gt;Results&lt;/h3&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-05-COSMIC/Relay_Events.png&quot; width=&quot;600&quot; /&gt;   &lt;figcaption&gt;
Figure 5. Bus voltage magnitudes when the branch from bus 6 to bus 9 in the 9-bus system is tripped. [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;A single-line outage was occurred at $t=10$ seconds, and  DIST relay timer activate.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$t_{preset-delay}=0.5$  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$P_1 (t=10.5)$: $t_{delay}$ ran out, and another line outage was occurred.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$P_2$: the magenta voltage trace violated the limit, and UVLS relay timer activate.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$P_3$: UVLS relay took action and shed 25% of the initial load at the bus.  &lt;br /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;c-cascading-outage-examples&quot;&gt;C. Cascading outage examples&lt;/h2&gt;

&lt;h3 id=&quot;purpose-2&quot;&gt;Purpose&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;To demonstrates how COSMIC processes cascading events such as line branch outages, load shedding, and is-landing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;used-test-systems-2&quot;&gt;Used test systems&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;39-bus system  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;2383-bus system  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;results-2&quot;&gt;Results&lt;/h3&gt;

&lt;h4 id=&quot;39-bus-case&quot;&gt;39-Bus Case&lt;/h4&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-05-COSMIC/39Bus_Case_Cascading_Outage_Example.png&quot; width=&quot;600&quot; /&gt;   &lt;figcaption&gt;
Table III. 39-Bus Case Cascading Outage Example [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;At  $t=3.00$ sec, the system suffered a strong dynamic oscillation after the initial two exogenous events (branches 2‚Äì25 and 5‚Äì6).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;At  $t=54.06$ sec, the first OC relay at branch 4‚Äì5 triggered.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;At  $t=55.06$ and $t=55.07$ sec, load shedding at two buses (Bus 7 and Bus 8) occurred.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;At $t=55.28$ sec, another two branches (10‚Äì13 and 13‚Äì14) shut down after OC relay trips. These events separated the system into two islands.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;At $t=55.78$ sec, two branches (3‚Äì4 and 17‚Äì18) were taken off the grid and this resulted in another island. The system eventually ended up with three isolated networks.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;2383-bus-case&quot;&gt;2383-Bus Case&lt;/h4&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-05-COSMIC/2383Bus_Case_Cascading_Outage_Example.png&quot; width=&quot;600&quot; /&gt;   &lt;figcaption&gt;
Figure 6. 2383-Bus Case Cascading Outage Example [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Number 0 with black highlights denotes the two initial events (N-2 contingency).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Other sequential numbers indicate the rest of the branch outages.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;In this example, 24 branches are off-line and cause a small island (within the dashed circle) in the end.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;The dots with additional red squares indicate buses where load shedding happens.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-05-COSMIC/Branch_Outage_and_Load_Shedding.png&quot; width=&quot;900&quot; /&gt;   &lt;figcaption&gt;
Figure 7. Branch outage events &amp;amp; Load-shedding events listed in Figure 8 [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The top panel in Figure 7 shows the timeline of all branch outage events for the 2383-bus cascading scenario, and the lower panel zooms in the load-shedding events.&lt;/p&gt;

&lt;p&gt;What we found here are ‚Ä¶&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the early phase of this cascading outages, the occurrence of the components failed relatively slowly, but it speeds up as the number of failures increased (check top panel).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;When the system condition was substantially compromised, fast collapse occurs and the majority of the branch undergo outages as well as the load shedding events (check lower panel).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;d-n-2-contingency-analysis&quot;&gt;D. N-2 contingency analysis&lt;/h2&gt;

&lt;h3 id=&quot;purpose-3&quot;&gt;Purpose&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;To studies the impact of different load modeling assumptions on cascading failure sizes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;used-test-systems-3&quot;&gt;Used test systems&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;2383-bus system&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;results-3&quot;&gt;Results&lt;/h3&gt;

&lt;h4 id=&quot;demand-loss&quot;&gt;Demand Loss&lt;/h4&gt;

&lt;p&gt;Figure 10 shows the complementary cumulative distribution function (CCDF) of demand losses for these four groups of simulations.&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-05-COSMIC/CCDF_Demand_Loss.png&quot; width=&quot;600&quot; /&gt;   &lt;figcaption&gt;
Figure 8. CCDF Demand Loss [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;The CCDF plots of demand losses exhibit a heavy-tailed blackout size distribution, which are typically found in both historical blackout data and cascading failure models [7].&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. WHY A HEAVY-TAILED DIST IS EASILY FOUND IN HISTORICAL DATA AND CASCADING FAILURE MODELS??? &lt;/span&gt;&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-05-COSMIC/Table_Event_Length.png&quot; width=&quot;600&quot; /&gt;   &lt;figcaption&gt;
Table IV. Average demand loss, average branch outages, and the probabilityies of loss for different load models [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;The constant Z load model ($Z_{100}I_0P_0E_0$) shows the best performance based on the average power loss and the probability of large blackout (listed in Table IV).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;As can be seen in Table IV, the probabilities of large demand losses varies from 2.5% to 3.5% for those four load configurations.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;These results show that load models play an important role in dynamic simulation and may increase the frequency of nonconvergence if they are not properly modeled.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. WHAT IS THE EXACT MEANDING OF NOT PROPERLY MODELED??? &lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;e-comparison-with-a-dc-cascading-outage-simulator&quot;&gt;E. Comparison with a dc cascading outage simulator&lt;/h2&gt;

&lt;h3 id=&quot;purpose-4&quot;&gt;Purpose&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;To compare results from COSMIC with results from a dc-power flow based model of cascading failure.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;It will help us to understand similarities and differences between these two different modeling approaches.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;used-test-systems-4&quot;&gt;Used test systems&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;2383-bus system&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;results-4&quot;&gt;Results&lt;/h3&gt;

&lt;h4 id=&quot;the-probabilities-of-demand-losses&quot;&gt;The probabilities of demand losses&lt;/h4&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-05-COSMIC/CCDF_COSMIC_vs_DC.png&quot; width=&quot;600&quot; /&gt;   &lt;figcaption&gt;
Figure 9. CCDF Demand Loss for COSMIC &amp;amp; DC Simulator [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;The probability of demand losses in the dc simulator is lower than that of COSMIC for the same amount of demand losses.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;The dc model is much &lt;strong&gt;more stable&lt;/strong&gt; and does not run into problems of numerical nonconvergence.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;COSMIC assumes that the network or sub-network in which the &lt;strong&gt;numerical failure&lt;/strong&gt; occurred experienced &lt;strong&gt;a complete blackout&lt;/strong&gt;.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;As a result, numerical failures in solving the DAE system greatly contributed to the larger blackout sizes. ‚Üí It may &lt;strong&gt;deteriorate the accuracy&lt;/strong&gt;.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;dc-model&quot;&gt;DC model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Pros  &lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;It is &lt;strong&gt;numerically stable&lt;/strong&gt;, making it possible to produce results that can be statistically similar to data from real power systems [8].  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cons  &lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;It includes &lt;strong&gt;numerous simplifications&lt;/strong&gt; that are substantially &lt;strong&gt;different from the ‚Äúreal‚Äù system&lt;/strong&gt;.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;dynamic-model&quot;&gt;Dynamic model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Pros  &lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;It &lt;strong&gt;includes many mechanisms&lt;/strong&gt; of cascading that cannot be represented in the dc model.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cons  &lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;It needs &lt;strong&gt;many assumptions&lt;/strong&gt; that are needed substantially impact the outcomes, potentially in ways that are not fully accurate.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;path-agreement-measurement-9&quot;&gt;Path Agreement Measurement [9]&lt;/h4&gt;

\[R(m_1, m_2) = \sum_{i=1}^{|C|}\frac{1}{C} \frac{|A_i \cap B_i|}{|A_i \cup B_i|} \tag{8}\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;models $m_1$ and $m_2$ are both subjected to the same set of exogenous contingencies $C=${$c_1,c_2,‚Ä¶$}.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;It measures the average agreement in the set of dependent events that result from each contingency in each model.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-05-COSMIC/PAM.png&quot; width=&quot;600&quot; /&gt;   &lt;figcaption&gt;
Table V. Statical Results for the Comparison between COSMIC &amp;amp; DC Simulator [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Table V shows that the average between the two models for the whole set of sequences is 0.1948, which means that there are substantial differences between cascade paths in the two models.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Part of the reason is that the dc model tends to produce longer cascades and consequently increase the denominator in (8).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. WHY DC MODEL TENDS TO PRODEUCE LONGER SCENARIOS??? &lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In order to control this, we computed only for &lt;strong&gt;the first ten branch outage events&lt;/strong&gt; (early stage).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;The average R increased to 0.3487, and some of the cascading paths showed a perfect match ($R=1$).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;This suggests that the cascading paths resulting from the &lt;strong&gt;two models tend to agree during the early stages of cascading&lt;/strong&gt;, when nonlinear dynamics are less pronounced, but disagree during later stages.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;4-conclusion&quot;&gt;4. CONCLUSION&lt;/h1&gt;

&lt;p&gt;Through the above experiments, the following conclusions are drawn.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;COSMIC&lt;/strong&gt; represents a power system as a set of &lt;strong&gt;hybrid discrete/continuous differential algebraic equations&lt;/strong&gt;, simultaneously simulating protection systems and machine dynamics.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;From the N‚àí2 contingency analysis, we found that COSMIC produces heavy-tailed blackout size distributions, which are typically found in both historical blackout data and cascading failure models [7] (Figure 8).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;From the N‚àí2 contingency analysis, we found that the blackout size results show that load models can substantially impact cascade sizes (Table IV).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;From the comparison with a dc cascading outage simulator, we found that the relative frequency of very large events may be exaggerated in dynamic model due to numerical non-convergence (about 3% of cases).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;From the comparison with a dc cascading outage simulator, we found that the two models largely agreed for the initial periods of cascading (for about 10 events), then diverged for later stages where dynamic phenomena drive the sequence of events.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Detailed dynamic models of cascading failure can be useful in understanding the relative importance of various features of these models.  &lt;br /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;5-my-reseasrch-project&quot;&gt;5. MY RESEASRCH PROJECT&lt;/h1&gt;

&lt;h2 id=&quot;purpose--goal&quot;&gt;Purpose / Goal&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Forecast or detect&lt;/strong&gt; (in real-time) cascading outages in power systems.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;approach&quot;&gt;Approach&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Use a GNN based model to predict or detect cascading outages.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;h3 id=&quot;step-1-data-generation&quot;&gt;Step 1. Data Generation&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Need to introduce &lt;strong&gt;randomness&lt;/strong&gt; to create large size of dataset.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Randomness will be given by the method that generated the uncertain loads in &lt;a href=&quot;https://arxiv.org/abs/1902.05607&quot;&gt;[10]&lt;/a&gt;:  &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

\[d := d + \mathcal{N}(\mu=0, \sigma = 0.03 * d)\]

    &lt;ul&gt;
      &lt;li&gt;$d$ is a load.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
      &lt;li&gt;$\mathcal{N}$ is normal distribution with mean zero and standard deviation proportional to the load.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;span style=&quot;color:green&quot;&gt; Q. WHICH FEATURES (e.g. $P_d, Q_d, P_g, Q_g$ or outage event time $t_{event}$) SHOULD BE GIVEN RANDOMNESS? &lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Codes  &lt;br /&gt;&lt;br /&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/jhyun0919/GNN-and-Power-Systems/blob/master/Cascading%20Outage/codes/gen_dataset_n_2.py&quot;&gt;Generate N-2 Contingency Datasets.py&lt;/a&gt;  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/jhyun0919/GNN-and-Power-Systems/blob/master/Cascading%20Outage/codes/1.%20Generate%20Datasets.ipynb&quot;&gt;Generate Datasets.ipynb&lt;/a&gt;  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/jhyun0919/GNN-and-Power-Systems/blob/master/Cascading%20Outage/codes/2.%20EDA.ipynb&quot;&gt;Exploratory Data Analysis.ipynb&lt;/a&gt;  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;step-2-design-the-gnn-model-architecture&quot;&gt;Step 2. Design the GNN Model Architecture&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Does the model &lt;strong&gt;ARCHITECTURE&lt;/strong&gt; is proper for the &lt;strong&gt;GOAL&lt;/strong&gt;?  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;What is the &lt;strong&gt;STRENGTH&lt;/strong&gt; of the model &lt;strong&gt;ARCHITECTURE&lt;/strong&gt;? and &lt;strong&gt;WHY&lt;/strong&gt;?  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;step-3-train--test-the-model&quot;&gt;Step 3. Train &amp;amp; Test the Model&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;step-4-tune-the-hyper-parameters&quot;&gt;Step 4. Tune the Hyper-parameters&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;ü§î&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;results-what-we-want-to-see&quot;&gt;Results (What we want to see‚Ä¶)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;High enough prediction or detection accuracy.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Extremely high computational efficiency compared to old methods.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;A GNN based model having significantly fewer nodes (trainable parameters) than other deep learning models.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;6-appendix&quot;&gt;6. APPENDIX&lt;/h1&gt;

&lt;h2 id=&quot;a-differential-equations-in-dynamic-power-system-1&quot;&gt;A. Differential equations in dynamic power system [1]&lt;/h2&gt;

&lt;h3 id=&quot;equation-for-rotor-speed&quot;&gt;Equation for rotor speed&lt;/h3&gt;

\[M \frac{dw_i}{dt} = P_{m,i} - P_{g,i} - D(w_i-1)\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\forall i \in N_G$ ($N_G$ is the set of all generator buses).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$M$ is a machine inertia constant.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$w_i$ is a rotor speed.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$P_{m,i}$ is the mechanical power input.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$P_{g,i}$ is the generator power output.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$D$ is a daping constant.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;equation-for-rotor-angle&quot;&gt;Equation for rotor angle&lt;/h3&gt;

\[\frac{d\delta_i(t)}{dt} = 2 \pi f_0 (w_i-1)\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$delta_i(t)$ is the rotor angle.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$f_0$ is the nominal frequency.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$w_i$ is fortor speed.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;etc.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;b-ac-power-flow-equation-11&quot;&gt;B. AC power flow equation [11]&lt;/h2&gt;

&lt;p&gt;Relation between following three compenets.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;real/reactive power&lt;/strong&gt; injected into each bus ($P_i, Q_i; i=1, 2, ‚Ä¶, n$)  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;bus voltage magnitude&lt;/strong&gt; of each bus ($V_i; i=1, 2, ‚Ä¶, n$)  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;phase angle&lt;/strong&gt; of each bus ($\theta_i; i=1, 2, ‚Ä¶, n$).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- $$
S_i = P_i + jQ_i\\
= \bar{V_i} \bar{I_i}^*\\
= \bar{V_i} ( \sum_{k=1}^{n} \bar{Y_{ik}} \bar{V_k} )^*\\
$$ --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

\[P_i = V_i\sum_{k=1}^{n}\{ G_{ik} cos(\theta_i-\theta_k) +B_{ik}sin(\theta_i-\theta_k) \}\]

\[Q_i = V_i\sum_{k=1}^{n}\{ G_{ik} sin(\theta_i-\theta_k) +B_{ik}cos(\theta_i-\theta_k) \}\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$n$ is the number of buses of the system.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;$Y_{ik} = G_{ik} + j B_{ik}$ is (i, k)-th entry of Y-bus matrix.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;references&quot;&gt;REFERENCES&lt;/h1&gt;

&lt;p&gt;[1] J. Song, E. Cotilla-Sanchez, G. Ghanavati and P. D. H. Hines, ‚ÄúDynamic Modeling of Cascading Failure in Power Systems,‚Äù in IEEE Transactions on Power Systems, vol. 31, no. 3, pp. 2085-2095, May 2016.&lt;/p&gt;

&lt;p&gt;[2] M. Papic et al., ‚ÄúSurvey of tools for risk assessment of cascading outages,‚Äù in Proc. IEEE Power and Energy Soc. General Meeting, 2011, pp. 1‚Äì9.&lt;/p&gt;

&lt;p&gt;[3] M. Vaiman et al., ‚ÄúRisk assessment of cascading outages: methodologies and challenges,‚Äù IEEE Trans. Power Syst., vol. 27, no. 2, pp. 631‚Äì641, May 2012.&lt;/p&gt;

&lt;p&gt;[4] I. Dobson, B. A. Carreras, V. E. Lynch, and D. E. Newman, ‚ÄúComplex systems analysis of series of blackouts: cascading failure, critical points, and self-organization,‚Äù Chaos: Interdisciplinary J. Nonlinear Sci., vol. 17, no. 2, p. 026103, 2007.&lt;/p&gt;

&lt;p&gt;[5] M. Eppstein and P. Hines, ‚ÄúA ‚Äòrandom chemistry‚Äô algorithm for identifying collections of multiple contingencies that initiate cascading failure,‚Äù IEEE Trans. Power Syst., vol. 27, no. 3, pp. 1698‚Äì1705, Aug. 2012.&lt;/p&gt;

&lt;p&gt;[6] ‚ÄúTrapezoidal Rule,‚Äù Math24, 30-Apr-2020. [Online]. Available: https://www.math24.net/trapezoidal-rule/. [Accessed: 07-Jan-2021].&lt;/p&gt;

&lt;p&gt;[7] P. Hines, J. Apt, and S. Talukdar, ‚ÄúLarge blackouts in North America: historical trends and policy implications,‚Äù Energy Policy, vol. 37, no. 12, pp. 5249‚Äì5259, 2009.&lt;/p&gt;

&lt;p&gt;[8] B. A. Carreras, D. E. Newman, I. Dobson, and N. S. Degala, ‚ÄúValidating OPA with WECC data,‚Äù in 2013 46th Hawaii Int. Conf. on System Sciences (HICSS), Jan. 2013, pp. 2197‚Äì2204.&lt;/p&gt;

&lt;p&gt;[9] R. Fitzmaurice, E. Cotilla-Sanchez, and P. Hines, ‚ÄúEvaluating the impact of modeling assumptions for cascading failure simulation,‚Äù in Proc. IEEE Power Energy Soc. General Meeting, Jul. 2012, pp. 1‚Äì8.&lt;/p&gt;

&lt;p&gt;[10] D. Deka and S. Misra, ‚ÄúLearning for DC-OPF: Classifying active sets using neural nets,‚Äù 2019 IEEE Milan PowerTech, Milan, Italy, 2019, pp. 1-6.&lt;/p&gt;

&lt;p&gt;[11] H.Zhu, ‚ÄúLecture Note - Ch 06‚Äù, The University of Texas at Austin, Austin, TX, EE 369: POWER SYSTEMS ENGINEERING Course, Fall 2019&lt;/p&gt;
</description>
        <pubDate>Tue, 05 Jan 2021 00:00:00 -0600</pubDate>
      </item>
    
      <item>
        <title>LMI and Power Systems</title>
        <link>/research/energy/2021/01/04/LMI-and-Power-Sys.html</link>
        <guid isPermaLink="true">/research/energy/2021/01/04/LMI-and-Power-Sys.html</guid>
        <description>
</description>
        <pubDate>Mon, 04 Jan 2021 00:00:00 -0600</pubDate>
      </item>
    
      <item>
        <title>Gram‚ÄìSchmidt Process</title>
        <link>/research/mathematics/2021/01/03/GS.html</link>
        <guid isPermaLink="true">/research/mathematics/2021/01/03/GS.html</guid>
        <description>&lt;p&gt;This article was written by referring to the &lt;em&gt;Fall 2020 Numerical Analysis: Linear Algebra Course&lt;/em&gt; and the following links.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.utexas.edu/users/flame/laff/alaff/chapter03-gram-schmidt.html&quot;&gt;ALAFF: Gram-Schmidt Orthogonalization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ULAFF/ALAFF&quot;&gt;ALAFF Git Repository&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;!-- ###################################################################### --&gt;

&lt;h1 id=&quot;gramschmidt-process&quot;&gt;Gram‚ÄìSchmidt Process&lt;/h1&gt;

&lt;p&gt;Given a set of linearly independent vectors $Span({ a_0, ‚Ä¶ , a_{n-1} } ) \subset \mathbb{C}^m$, the Gram-Schimdt process computes a new basis {$q_0, ‚Ä¶ , q_{n-1}$} that &lt;strong&gt;spans the same subspace&lt;/strong&gt; as the original vectors, i.e. $Span({ a_0, ‚Ä¶ , a_{n-1} } ) = Span({ q_0, ‚Ä¶ , q_{n-1} } )$ [1].&lt;/p&gt;

&lt;p&gt;It should be noted that we would like to have the new basis, ${ q_0, ‚Ä¶ , q_{n-1} }$, having the following characteristics.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All vectors are &lt;strong&gt;unit vectors&lt;/strong&gt; of length 1, i.e. $|q_i| = 1$.&lt;/li&gt;
  &lt;li&gt;All vectors are &lt;strong&gt;mutually perpendicular&lt;/strong&gt;, i.e. $q_i \perp q_j$ where $i \neq j$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These characteristics can be obtained through the following routine which is composed by three steps [1]:&lt;/p&gt;

&lt;blockquote&gt;

  &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;Gram-Schmidt Process&amp;gt;
  Step 1. Compute the direction of the vectors.
  Step 2. Compute the magnitude of the normalizer.
  Step 3. Compute a unit vector.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Compute the vector $q_0$ ($i=0$)
    &lt;ul&gt;
      &lt;li&gt;Step 1. Compute the direction of the vectors.&lt;/li&gt;
    &lt;/ul&gt;

\[N.A. \ (q_0 \parallel a_0)\]

    &lt;ul&gt;
      &lt;li&gt;Stpe 2. Compute the magnitude of the normalizer.&lt;/li&gt;
    &lt;/ul&gt;

\[\rho_{0, 0}=\|a_0\|_2\]

    &lt;ul&gt;
      &lt;li&gt;Step 3. Compute a unit vector.&lt;/li&gt;
    &lt;/ul&gt;

\[q_0=a_0/\rho_{0, 0}\]

    &lt;ul&gt;
      &lt;li&gt;Result&lt;/li&gt;
    &lt;/ul&gt;

\[a_0=q_0\rho_{0, 0}\]
  &lt;/li&gt;
  &lt;li&gt;Compute the vector $q_1$ ($i=1$)
    &lt;ul&gt;
      &lt;li&gt;Step 1. Compute the direction of the vectors.&lt;/li&gt;
    &lt;/ul&gt;

\[\rho_{0, 1}=q_0^{H}a_1\]

\[a_{1}^{\perp}=a_1-\rho_{0, 1}q_0\]

    &lt;ul&gt;
      &lt;li&gt;Step 2. Compute the magnitude of the normalizer.&lt;/li&gt;
    &lt;/ul&gt;

\[\rho_{1, 1}=\|a_1^\perp\|_2\]

    &lt;ul&gt;
      &lt;li&gt;Step 3. Compute a unit vector.&lt;/li&gt;
    &lt;/ul&gt;

\[q_1=a_1^\perp/\rho_{1, 1}\]

    &lt;ul&gt;
      &lt;li&gt;Result&lt;/li&gt;
    &lt;/ul&gt;

\[\left[\begin{array}{c|c}a_{0} &amp;amp; a_{1}\end{array}
      \right]=
      \left[\begin{array}{c|c}q_{0} &amp;amp; q_{1}\end{array}
      \right]
      \left[\begin{array}{c|c}
      \rho_{0, 0} &amp;amp; \rho_{0, 1} \\
      \hline
      0 &amp;amp; \rho_{1, 1}
      \end{array}
      \right]\]
  &lt;/li&gt;
  &lt;li&gt;Compute the vector $q_2$ ($i=2$)
    &lt;ul&gt;
      &lt;li&gt;Step 1. Compute the direction of the vectors.&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{bmatrix}\rho_{0, 2} \\ \rho_{1, 2}\end{bmatrix}=\begin{bmatrix}q_{0} &amp;amp; q_{1}\end{bmatrix}^Ha_2\]

\[a_{2}^{\perp}=a_2-\begin{bmatrix}q_{0} &amp;amp; q_{1}\end{bmatrix}\begin{bmatrix}\rho_{0, 2} \\ \rho_{1, 2}\end{bmatrix}\]

    &lt;ul&gt;
      &lt;li&gt;Step 2. Compute the magnitude of the normalizer.&lt;/li&gt;
    &lt;/ul&gt;

\[\rho_{2, 2}=\|a_2^\perp\|_2\]

    &lt;ul&gt;
      &lt;li&gt;Step 3. Compute a unit vector.&lt;/li&gt;
    &lt;/ul&gt;

\[q_2=a_2^\perp/\rho_{2, 2}\]

    &lt;ul&gt;
      &lt;li&gt;Result&lt;/li&gt;
    &lt;/ul&gt;

\[\left[\begin{array}{c|c}a_{0} \ a_{1}&amp;amp; a_{2}\end{array}
      \right]=
      \left[\begin{array}{c|c}q_{0} \ q_{1} &amp;amp; q_{2}\end{array}
      \right]
      \left[\begin{array}{cc|c}
      \rho_{0, 0}  &amp;amp; \rho_{0, 1} &amp;amp; \rho_{0, 2} \\
      0 &amp;amp; \rho_{1, 1} &amp;amp; \rho_{1, 2}\\
      \hline
      0 &amp;amp; 0 &amp;amp; \rho_{2, 2}
      \end{array}
      \right]\]
  &lt;/li&gt;
  &lt;li&gt;keep do the routine till $i=n-1$
    &lt;ul&gt;
      &lt;li&gt;Step 1. Compute the direction of the vectors.&lt;/li&gt;
      &lt;li&gt;Step 2. Compute the magnitude of the normalizer.&lt;/li&gt;
      &lt;li&gt;Step 3. Compute a unit vector.&lt;/li&gt;
      &lt;li&gt;Final Result&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

\[A = QR\]

&lt;p&gt;What we should pay attention to in the final result ($A = QR$) are ‚Ä¶&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All column vectors of $Q$, the new basis {$q_0, ‚Ä¶ , q_{n-1}$}, are unit vector and mutually orthogonal, which means $Q$ is an &lt;strong&gt;unitary matrix&lt;/strong&gt;.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;The matrix $R$ is &lt;strong&gt;upper triangular matrix&lt;/strong&gt; and $rank(R)=m$.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;The dot product of $Q$ and $R$, which is equal to $A$, can be regarded as a &lt;strong&gt;linear combination&lt;/strong&gt; of $Q$‚Äôs column vectors.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;The result of &lt;strong&gt;Gram-Shmidt Process&lt;/strong&gt; can be regarded as a result of &lt;strong&gt;QR factorization&lt;/strong&gt; (decomposotion).  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;!-- ###################################################################### --&gt;

&lt;h1 id=&quot;classical-gram-schmidt-cgs&quot;&gt;Classical Gram-Schmidt (CGS)&lt;/h1&gt;

&lt;p&gt;The content discussed in the previous section can be summarized into an algorithm as follows, which is the CGS-QR algorithm [2].&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Consider $A=QR$.&lt;/p&gt;

&lt;p&gt;Partition the given matrices as follows.&lt;/p&gt;

\[\left[\begin{array}{c|cc}A_{0} &amp;amp; a_{1} &amp;amp; A_{2}\end{array}
        \right]=
        \left[\begin{array}{c|cc}Q_{0} &amp;amp; q_{1} &amp;amp; Q_{2}\end{array}
        \right]
        \left[\begin{array}{c|cc}
        R_{0, 0}  &amp;amp; r_{0, 1} &amp;amp; R_{0, 2} \\
        \hline
        0 &amp;amp; \rho_{1, 1} &amp;amp; r_{1, 2}^T\\
        0 &amp;amp; 0 &amp;amp; R_{2, 2}
        \end{array}
        \right]\]

&lt;p&gt;Assume that $Q_0$ and $R_{0,0}$ have already been computed.&lt;/p&gt;

&lt;p&gt;Since $Q$ is unitary matric ($Q_0^HQ_0=I$ and $Q_0^Hq_1=0$),&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$Q_0^Ha_1=Q_0^HQ_0r_{0,1}+q_1\rho_{1,1}=r_{01}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Compute $r_{0,1}$, which is already known.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$r_{0,1}:=Q_0^Ha_1$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Compute $a_1^\perp$ and $\rho_{1,1}$ (Step 1. &amp;amp; 2.).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$a_1^\perp:=a_1-Q_0r_{0,1}$&lt;/li&gt;
  &lt;li&gt;$\rho_{1,1}:=|a_1^\perp|$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Compute $q_1$ (Step 3.).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$q_1 := a_1^\perp / \rho_{1,1}$&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Figure 1. shows Classical Gram-Schmidt algorithm for computing the QR factorization of a matrix A. The algorithm used &lt;a href=&quot;https://github.com/jhyun0919/ALAFF-in-python/blob/main/Assignments/flame_tests.ipynb&quot;&gt;FLAME notation&lt;/a&gt;.&lt;br /&gt;
Code 1. shows the algorithms in python language.&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2020-12-26-GS/CGS.png&quot; width=&quot;70%&quot; /&gt;   &lt;figcaption&gt;
Figure 1. Classical Gram-Schmidt algorithm for computing the QR factorization of a matrix A [2]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;  &lt;script src=&quot;https://gist.github.com/jhyun0919/79fefe26187d174e154474e64516a8f1.js&quot;&gt;&lt;/script&gt;
 &lt;em&gt;Code. 1: CGS QR in python&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jhyun0919/ALAFF-in-python/blob/main/Assignments/week03/test_CGS_QR.ipynb&quot;&gt;Test_CGS_QR.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;!-- ###################################################################### --&gt;

&lt;h1 id=&quot;modified-gram-schmidt-mgs&quot;&gt;Modified Gram-Schmidt (MGS)&lt;/h1&gt;

&lt;p&gt;Gram-Schmidt process can be performed differently from CGS, and the corresponding algorithm is as follows, which is called Modified Gram-Schmidt (MGS) [3].&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Consider $A=QR$.&lt;/p&gt;

&lt;p&gt;Partition the given matrices as follows.&lt;/p&gt;

\[\left[\begin{array}{c|cc}A_{0} &amp;amp; a_{1} &amp;amp; A_{2}\end{array}
        \right]=
        \left[\begin{array}{c|cc}Q_{0} &amp;amp; q_{1} &amp;amp; Q_{2}\end{array}
        \right]
        \left[\begin{array}{c|cc}
        R_{0, 0}  &amp;amp; r_{0, 1} &amp;amp; R_{0, 2} \\
        \hline
        0 &amp;amp; \rho_{1, 1} &amp;amp; r_{1, 2}^T\\
        0 &amp;amp; 0 &amp;amp; R_{2, 2}
        \end{array}
        \right]\]

&lt;p&gt;Assume that $a_1$ and $A_2$ are known.&lt;/p&gt;

&lt;p&gt;Compute $\rho_{1,1}$ (compute the magnitude ‚ÜîÔ∏é Step 2.).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\rho_{1,1}:=|a_1|$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Compute $q_1$ (compute the unit vector ‚ÜîÔ∏é Step 3.).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$q_1:=a_1/\rho_{1,1}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Compute $r_{0,1}^T$, which is already known.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$r_{0,1}^T:=q_1^H(A_2 - Q_2R_{22}) = q_1^HA_2$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Compute $A_2$ (update $A_2$ for the orthogonality ‚ÜîÔ∏é Step 1.).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$A_2:=A_2-q_1r_{0,1}^T$&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Figure 2. shows Modified Gram-Schmidt algorithm for computing the QR factorization of a matrix A. The algorithm used &lt;a href=&quot;https://github.com/jhyun0919/ALAFF-in-python/blob/main/Assignments/flame_tests.ipynb&quot;&gt;FLAME notation&lt;/a&gt;.&lt;br /&gt;
Code 2. shows the algorithms in python language.&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2020-12-26-GS/MGS-side-by-side.png&quot; width=&quot;100%&quot; /&gt;   &lt;figcaption&gt;
Figure 2. Alternative Modified Gram-Schmidt algorithm for computing the QR factorization of a matrix A [3]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;  &lt;script src=&quot;https://gist.github.com/jhyun0919/c1395732ab7336b7a27b719af66213ab.js&quot;&gt;&lt;/script&gt;
 &lt;em&gt;Code. 2: MGS QR in python&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/jhyun0919/ALAFF-in-python/blob/main/Assignments/week03/test_MGS_QR.ipynb&quot;&gt;Test_MGS_QR.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;!-- ###################################################################### --&gt;

&lt;h1 id=&quot;why-do-we-need-this&quot;&gt;Why do we need this?&lt;/h1&gt;

&lt;p&gt;Through Gram-Schmidt process, we can obtain an &lt;strong&gt;orthonormal basis&lt;/strong&gt; of the matrix $A$‚Äôs subspace, which has a set of lineary independent vectors {$a_0, ‚Ä¶, a_{n-1}$}.&lt;/p&gt;

&lt;p&gt;The advantages of being able to have an orthonormal basis are following [4].&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We can express any  $v \in \mathbb{C}^n$ as a &lt;strong&gt;linear combination of coefficients&lt;/strong&gt;, which will allows us to have an &lt;strong&gt;explicit formula&lt;/strong&gt; expressing $v$ with the orthonormal basis.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;The explicit formula is very useful when dealing with &lt;strong&gt;projection onto subspace&lt;/strong&gt;.  &lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;
&lt;p&gt;[1] M. M. Robert van de Geijn, ‚ÄúAdvanced Linear Algebra: Foundations to Frontiers,‚Äù ALAFF Classical Gram-Schmidt (CGS). [Online]. Available: https://www.cs.utexas.edu/users/flame/laff/alaff/chapter03-classical-gram-schmidt.html. [Accessed: 03-Jan-2021].&lt;/p&gt;

&lt;p&gt;[2] M. M. Robert van de Geijn, ‚ÄúAdvanced Linear Algebra: Foundations to Frontiers,‚Äù ALAFF Classical Gram-Schmidt algorithm. [Online]. Available: https://www.cs.utexas.edu/users/flame/laff/alaff/chapter03-cgs-algorithm.html. [Accessed: 03-Jan-2021].&lt;/p&gt;

&lt;p&gt;[3] M. M. Robert van de Geijn, ‚ÄúAdvanced Linear Algebra: Foundations to Frontiers,‚Äù ALAFF Modified Gram-Schmidt (MGS). [Online]. Available: https://www.cs.utexas.edu/users/flame/laff/alaff/Modified-Classical-Gram-Schmidt.html. [Accessed: 03-Jan-2021].&lt;/p&gt;

&lt;p&gt;[4] Michael Albanese, ‚ÄúWhy is orthogonal basis important?,‚Äù Mathematics Stack Exchange. [Online]. Available: https://math.stackexchange.com/questions/518600/why-is-orthogonal-basis-important. [Accessed: 03-Jan-2021].&lt;/p&gt;
</description>
        <pubDate>Sun, 03 Jan 2021 00:00:00 -0600</pubDate>
      </item>
    
      <item>
        <title>(Python-Dev Tips) Virtual Environment</title>
        <link>/research/cs/2021/01/01/virtualenv.html</link>
        <guid isPermaLink="true">/research/cs/2021/01/01/virtualenv.html</guid>
        <description>&lt;p&gt;In this article, we are going to explain what is virtual environment in python and why do we need it. Also, we will show you how to set up a virtual environment and introduce basic commands.&lt;/p&gt;

&lt;p&gt;This posting is one of a series of tips for developing python following the previous post.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://jhyun0919.github.io/research/cs/2020/10/01/git-tips.html&quot;&gt;Git Basic&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Virtual Environment&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jhyun0919.github.io/research/cs/2020/10/03/pep8.html&quot;&gt;PEP8&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jhyun0919.github.io/research/cs/2020/10/04/jupyter.html&quot;&gt;Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This post was written with reference to the following materials.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.python.org/3/tutorial/venv.html&quot;&gt;Python Docs: Virtual Environments&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://realpython.com/python-virtual-environments-a-primer/&quot;&gt;Real Python: Python Virtual Environments&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;what-is-virtual-env--why-do-we-need-it&quot;&gt;What is Virtual-Env &amp;amp; Why Do We Need it?&lt;/h1&gt;

&lt;p&gt;Python &lt;strong&gt;virtual environment&lt;/strong&gt; is an isolated environment for a Python project. This allows each project can have its &lt;strong&gt;own dependencies&lt;/strong&gt;, regardless of what dependencies every other project has [1].&lt;/p&gt;

&lt;p&gt;This provides the advantage of being able to build each development environment for multiple projects on one local PC. For an example, Figure 1. shows how one can manage virtual environments for serveral projects. More specifically, it shows how to manage each python projection for a web app that uses a different version of the Django package.&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-04-virtualenv/virtualenv_concept.png&quot; width=&quot;555&quot; /&gt;   &lt;figcaption&gt;
Figure 1. Examples of Python Virtual Environment [2].
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;basic-virtual-env-usages&quot;&gt;Basic Virtual-Env Usages&lt;/h1&gt;

&lt;p&gt;There are two ways to set up and manage virtual envoronment, &lt;strong&gt;pip&lt;/strong&gt; and &lt;strong&gt;anaconda&lt;/strong&gt;. In this article, we will explain how to set up and manage virtual envoronment using anaconda.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.anaconda.com&quot;&gt;&lt;strong&gt;Anaconda&lt;/strong&gt;&lt;/a&gt; is a package manager, an environment manager, a Python/R data science distribution, and a collection of over 7,500+ open-source packages [3]. Through this, we can set up and manage a virtual emvironment for each projects.&lt;/p&gt;

&lt;p&gt;We will introduce basic anaconda usages. For more details, you can refer to the following Documentation or Cheat-Sheet.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.anaconda.com/anaconda/&quot;&gt;Anaconda Documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://conda.io/projects/conda/en/latest/user-guide/cheatsheet.html&quot;&gt;Conda Cheat-Sheet&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;We can create a new virtual environment with the following commands and manage packages in the corresponding virtual environment for each project.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Create a new virtual environmnet  &lt;br /&gt;&lt;br /&gt;
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;conda create &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; ENVNAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Activate a named environment  &lt;br /&gt;&lt;br /&gt;
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;conda activate ENVNAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Install a package  &lt;br /&gt;&lt;br /&gt;
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;conda &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;PKGNAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Deactivate current environment  &lt;br /&gt;&lt;br /&gt;
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;conda deactivate ENVNAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;And, we can clearly provide the dependency for the project to other developers through the following commands.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Exporting the &lt;em&gt;envname.yml&lt;/em&gt;  &lt;br /&gt;&lt;br /&gt;
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;conda &lt;span class=&quot;nb&quot;&gt;env export&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; ENVNAME &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; envname.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Finally, we can create a virtual environment that matches the developer‚Äôs development environment on our local PC through the &lt;em&gt;envname.yml&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Creating an environment from an &lt;em&gt;envname.yml&lt;/em&gt;  &lt;br /&gt;&lt;br /&gt;
    &lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;conda &lt;span class=&quot;nb&quot;&gt;env &lt;/span&gt;create &lt;span class=&quot;nt&quot;&gt;--file&lt;/span&gt; envname.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;!-- &lt;figure align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://jhyun0919.github.io/assets/img/2021-01-04-virtualenv/anaconda_gui.png&quot; width=&quot;900&quot; /&gt;
  &lt;figcaption&gt;Figure 2. Example of Anaconda GUI.&lt;/figcaption&gt;
&lt;/figure&gt; --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;
&lt;p&gt;[1] Real Python, ‚ÄúPython Virtual Environments: A Primer,‚Äù Real Python, 17-Jul-2020. [Online]. Available: https://realpython.com/python-virtual-environments-a-primer/. [Accessed: 05-Jan-2021].&lt;/p&gt;

&lt;p&gt;[2] S. Shakya, ‚ÄúVirtual Environment in Python,‚Äù Medium, 02-May-2019. [Online]. Available: https://medium.com/incwell-bootcamp/virtual-environment-in-python-54db665b9939. [Accessed: 05-Jan-2021].&lt;/p&gt;

&lt;p&gt;[3] ‚ÄúAnaconda Individual Edition¬∂,‚Äù Anaconda Individual Edition - Anaconda documentation. [Online]. Available: https://docs.anaconda.com/anaconda/. [Accessed: 05-Jan-2021].&lt;/p&gt;
</description>
        <pubDate>Fri, 01 Jan 2021 00:00:00 -0600</pubDate>
      </item>
    
      <item>
        <title>(TIL) How to Speak</title>
        <link>/daily/essay/2020/12/28/how-to-speak.html</link>
        <guid isPermaLink="true">/daily/essay/2020/12/28/how-to-speak.html</guid>
        <description>&lt;p&gt;&lt;a href=&quot;http://sixminutes.dlugan.com/speaking-tips-patrick-henry-winston-speak/&quot;&gt;How to Speak: 7 Speaking Tips from Patrick Henry Winston&lt;/a&gt;
&lt;a href=&quot;https://www.youtube.com/watch?v=Unzc731iCUY&quot;&gt;How To Speak by Patrick Winston&lt;/a&gt;
&lt;a href=&quot;https://www.youtube.com/watch?v=uylHeDdlMrc&amp;amp;amp%3Bab_channel=sergiusheartless&quot;&gt;How to Speak - The Big Four&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 28 Dec 2020 00:00:00 -0600</pubDate>
      </item>
    
      <item>
        <title>(Tutorial) EMTP - Line/Cable Modeling for EMT Simulations</title>
        <link>/research/energy/2020/12/14/EMTP-Line-Cable-Modeling.html</link>
        <guid isPermaLink="true">/research/energy/2020/12/14/EMTP-Line-Cable-Modeling.html</guid>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/playlist?list=PLP4K-zG4OPTEWlzPJQvKxnDd1I0ObW4MS&quot;&gt;EMTP Tutorials&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Mon, 14 Dec 2020 00:00:00 -0600</pubDate>
      </item>
    
      <item>
        <title>(TIL) Reviewing Statictics and Probability</title>
        <link>/research/mathematics/2020/12/13/prob.html</link>
        <guid isPermaLink="true">/research/mathematics/2020/12/13/prob.html</guid>
        <description>
</description>
        <pubDate>Sun, 13 Dec 2020 00:00:00 -0600</pubDate>
      </item>
    
      <item>
        <title>La√Øcit√©</title>
        <link>/daily/essay/2020/12/12/essay.html</link>
        <guid isPermaLink="true">/daily/essay/2020/12/12/essay.html</guid>
        <description>&lt;p&gt;Recently, I have finished two final exams, and one left next Thursday. I think it is a good moment to start again to write what I have been thinking these days. Looking back at the end of the semester, I realized that a lot had happened globally, which I have not thought deeply about due to my hectic school life. For instance, the pandemic broke out, and we are living in the era of New Normal. Also, there was a presidential election in the US. In my thought, these are the most notable or significant events of this year. However, in this article, I would like to share my view about what happened in France.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://bdidier.fr/wp-content/uploads/1250_portail_laic.jpg&quot; width=&quot;90%&quot; /&gt;   &lt;figcaption align=&quot;center&quot;&gt;
(https://bdidier.fr/4eme-la-laicite/)
  &lt;/figcaption&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In October, a French middle school teacher was attacked and beheaded by a terrorist. Similar incidents have been happening after this incident, and it reminded me of the terror in Paris four years ago. Behind these tragedies, we all know there are complex religious problems. Several factors may be responsible for the continued occurrence of these incidents in France. In my respect, it has a lot to do with Laicite (Secularism).&lt;/p&gt;

&lt;p&gt;I know that it is nearly impossible for me, not French, to fully understand and describe the meaning of Laicite. To the best of my understanding, it means strict separation of religion from society, the secular world. There must be pros and cons of separating religion from our lives. I can agree that we can get an opportunity to consider religion on an equal footing with other values by taking it apart from society. For example, in Korea, many parents agree that their children can make decisions for their future by themselves. However, suddenly they change extremely conservative when it comes to religious choices, which is nonsense, given that it is just another decision like others. I think they act differently because they never take apart religion from their lives and always give a top priority to it. Suppose parents had tried to separate religion from society and consider it on an equal footing. In that case, they might have different conversations with their children and ended up with more rational conclusions.&lt;/p&gt;

&lt;p&gt;However, a series of these thoughts can be taken as a challenge or blasphemy for violent and radical religious groups. In my opinion, these groups are very likely to use violence in the form of retaliation when society becomes unstable. As a result, ironically, the society where rational judgment can be made is exposed to the risk of terror. As far as religion is concerned, I think France has a society with good conditions for making rational judgments. Simultaneously, the country is exposed to violence in an unstable period due to the pandemic. I hope they will overcome this crisis well without further tragedy, and others will be able to learn the unique way of their thinking.&lt;/p&gt;
</description>
        <pubDate>Sat, 12 Dec 2020 00:00:00 -0600</pubDate>
      </item>
    
      <item>
        <title>(Python-Dev Tips) Git Basic</title>
        <link>/research/cs/2020/10/01/git-tips.html</link>
        <guid isPermaLink="true">/research/cs/2020/10/01/git-tips.html</guid>
        <description>&lt;p&gt;Starting with this posting, we will upload a series of articles that can help with Python development. The series will be written in four articles, and the contents are as follows, and if necessary, additional posts will be uploaded as a separate series.&lt;/p&gt;

&lt;p&gt;In this article, we will show you what Git is and Why do we need it. Also, we will give a brief intro to how we can start and use Git.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Git Basic&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jhyun0919.github.io/research/cs/2021/01/01/virtualenv.html&quot;&gt;Virtual Environment&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jhyun0919.github.io/research/cs/2020/10/03/pep8.html&quot;&gt;PEP8&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jhyun0919.github.io/research/cs/2020/10/04/jupyter.html&quot;&gt;Jupyter&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;what-is-git&quot;&gt;What is Git&lt;/h1&gt;

&lt;p&gt;Git is a free and open-source distributed &lt;strong&gt;version control&lt;/strong&gt; system designed to handle everything from small to very large projects with speed and efficiency [1]. It provides us a history of content changes and facilitates collaborative changes to files [2].&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2020-10-01-git/git.jpeg&quot; width=&quot;80%&quot; /&gt;   &lt;figcaption&gt;
Figure 1. Examples of Using Git [3].
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;why-do-we-need-git&quot;&gt;Why Do We Need Git&lt;/h1&gt;
&lt;p&gt;When we work on a project and do something creative and productive, we repeat the following four steps [2].&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create things.&lt;/li&gt;
  &lt;li&gt;Save things.&lt;/li&gt;
  &lt;li&gt;Edit things.&lt;/li&gt;
  &lt;li&gt;Save the things again.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;By repeating the above steps, we add new things, delete unnecessary ones, request modifications, and make corrections where necessary. To make this process straightforward and efficient, we turn to the help of &lt;strong&gt;version control&lt;/strong&gt;. Through the version control system, we can easily track the followings [2].&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When we did it.&lt;/li&gt;
  &lt;li&gt;Why we did it.&lt;/li&gt;
  &lt;li&gt;What the contents of the change.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This makes it possible to efficiently understand the project‚Äôs process and be more productive when revisiting the project in the future.&lt;/p&gt;

&lt;p&gt;For these reasons, we cannot overemphasize the necessity of version control when working on projects through collaboration as well as personal projects. By using &lt;strong&gt;Git&lt;/strong&gt;, we can take these benefits of version control.&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2020-10-01-git/gitflow.png&quot; width=&quot;80%&quot; /&gt;   &lt;figcaption&gt;
Figure 2. Workflow with Git [4].
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;basic-git-commands&quot;&gt;Basic Git Commands&lt;/h1&gt;

&lt;p&gt;The following are essential terminal commands for Git. It may be difficult to memorize, but it is good to understand only its purpose. As long as you understand each command‚Äôs purpose, you can easily use Git through a GUI, which will be covered in the following section.&lt;/p&gt;

&lt;p&gt;For reference, the contents are referred to as the following links.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dev.to/dhruv/essential-git-commands-every-developer-should-know-2fl&quot;&gt;Essential git commands every developer should know&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dzone.com/articles/top-20-git-commands-with-examples&quot;&gt;Top 20 Git Commands With Examples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;git-init&quot;&gt;git init&lt;/h2&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git init &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;project-name]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command is used to start a new repository.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;git-clone&quot;&gt;git clone&lt;/h2&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git clone &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;url]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command is used to obtain a repository from an existing URL.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;git-add&quot;&gt;git add&lt;/h2&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git add &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;file-name]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command adds a file to the staging area.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;git-rm&quot;&gt;git rm&lt;/h2&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;file-name]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command deletes the file from your working directory and stages the deletion.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;git-commit&quot;&gt;git commit&lt;/h2&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git commit &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; ‚ÄúMessage to go with the commit here‚Äù
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command records or snapshots the file permanently in the version history.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;git-status&quot;&gt;git status&lt;/h2&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command lists all the files that have to be committed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;git-push&quot;&gt;git push&lt;/h2&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git push &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;variable name] master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command sends the committed changes of master branch to your remote repository.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git push &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;variable name] &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;branch-name]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command sends the branch commits to your remote repository.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;git-branch&quot;&gt;git branch&lt;/h2&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git branch
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command lists all the local branches in the current repository.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git branch &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;branch-name]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command creates a new branch.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git branch &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;branch-name]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command deletes the feature branch.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;git-checkout&quot;&gt;git checkout&lt;/h2&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git checkout &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;branch-name]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command is used to switch from one branch to another.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git checkout &lt;span class=&quot;nt&quot;&gt;-b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;branch-name]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command creates a new branch and also switches to it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;git-pull&quot;&gt;git pull&lt;/h2&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git pull &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;repository &lt;span class=&quot;nb&quot;&gt;link&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command fetches and merges changes on the remote server to your working directory.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;git-merge&quot;&gt;git merge&lt;/h2&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git merge &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;branch-name]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command merges the specified branch‚Äôs history into the current branch.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;git-diff&quot;&gt;git diff&lt;/h2&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git diff
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command shows the file differences which are not yet staged.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git diff &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;first branch-name] &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;second branch-name]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command shows the differences between the two branches mentioned.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;git-stash&quot;&gt;git stash&lt;/h2&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git stash save
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command temporarily stores all the modified tracked files.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git stash pop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command restores the most recently stashed files.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git stash list
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command lists all stashed changesets.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git stash drop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;This command discards the most recently stashed changeset.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;gitkraken&quot;&gt;GitKraken&lt;/h1&gt;

&lt;p&gt;There is an entry barrier to use the commands described above in a terminal window. For those who have difficulty using Git in a terminal window, we suggest to use a &lt;strong&gt;GUI&lt;/strong&gt; called &lt;strong&gt;GitKraken&lt;/strong&gt;. With GitKraken, you can intuitively and easily manage projects without memorizing all the git commands. Figure 3. is a screenshot of GitKraken.&lt;/p&gt;

&lt;p&gt;We can do the basic operation as the git commands described above through the buttons in Area 1.
In Area 2, GitKraken shows the files that have been modified compared to the log, whether these files are added in the staging area, and allows us to commit.
Lastly, in Area 3, it shows us the workflow of the project.&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2020-10-01-git/gitkraken eg.png&quot; width=&quot;100%&quot; /&gt;   &lt;figcaption&gt;
Figure 3. GitKraken Screenshot.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;one-more-thing-&quot;&gt;One More Thing üòú&lt;/h1&gt;

&lt;p&gt;Those who want to use Git only with commands in a terminal window, not using a GUI, or who want to have a deeper understanding, can learn without losing interest through the quest-game-type tutorial provided at the following link.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://learngitbranching.js.org/?locale=en&quot;&gt;Git Tutorial Quest Game&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2020-10-01-git/git tutorial.gif&quot; width=&quot;90%&quot; /&gt;   &lt;figcaption&gt;
Figure 4. Git Command Line Tutorial Screenshot.
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;
&lt;p&gt;[1] Git. [Online]. Available: https://git-scm.com/. [Accessed: 01-Oct-2020].&lt;/p&gt;

&lt;p&gt;[2] ‚ÄúGit Basics Episode 1,‚Äù Git. [Online]. Available: https://git-scm.com/video/what-is-version-control. [Accessed: 01-Oct-2020].&lt;/p&gt;

&lt;p&gt;[3] S. C. Atuonwu, ‚Äú5 Git Commands You Should Know, with Code Examples,‚Äù freeCodeCamp.org, 10-Jun-2020. [Online]. Available: https://www.freecodecamp.org/news/5-git-commands-you-should-know-with-code-examples/. [Accessed: 01-Oct-2020].&lt;/p&gt;

&lt;p&gt;[4] Atlassian, ‚ÄúGitflow Workflow: Atlassian Git Tutorial,‚Äù Atlassian. [Online]. Available: https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow. [Accessed: 01-Oct-2020].&lt;/p&gt;
</description>
        <pubDate>Thu, 01 Oct 2020 00:00:00 -0500</pubDate>
      </item>
    
      <item>
        <title>The shape of someone or something that I love</title>
        <link>/daily/essay/2020/08/18/essay.html</link>
        <guid isPermaLink="true">/daily/essay/2020/08/18/essay.html</guid>
        <description>&lt;p&gt;Whenever I come back to Korea, Îñ°Î≥∂Ïù¥ has always been the first food that I had. Briefly speaking, this food is just some rice cakes with spicy sauce. Compare to other Korean foods, Îñ°Î≥∂Ïù¥ might seem shabby. However, this food is one of the reasons why I love and how I remember this place. Why did I pick Îñ°Î≥∂Ïù¥ instead of other fancy dishes, and why am I so into it? I guess this is because it is my unique way of engraving memories in this country.&lt;/p&gt;

&lt;p&gt;Likewise, most people who have not grown up in this country might come up with great landmarks when they imagine Korea. However, as a born and raised person in Korea, the shape of this country in my heart is not like that. It could be the small marketplace in my old hometown, or it could be the parking lot in front of my apartment building, where I used to ride a bike and roller skates. It is kind of a secret between me and the place that I love.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://media.giphy.com/media/9G0AdBbVrkV3O/giphy.gif&quot; width=&quot;444&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This pattern is identically applied to the people around me. The one who I love (or loved) is not remembered as her bright and shining moments that all the others could have seen. I memorized her in a way that no one could imagine. It does not have to be pretty or lovely. It just needs to be my little secret between her and me. These secrets comfort me and give strength, even though I can no longer be together with her.&lt;/p&gt;
</description>
        <pubDate>Tue, 18 Aug 2020 00:00:00 -0500</pubDate>
      </item>
    
      <item>
        <title>Geometric Meaning of Hessian Matrix</title>
        <link>/research/mathematics/2020/07/21/geometric-meaning-of-hessian.html</link>
        <guid isPermaLink="true">/research/mathematics/2020/07/21/geometric-meaning-of-hessian.html</guid>
        <description>&lt;p&gt;Starting with the definition of the Hessian Matrix, this posting will focus on the geometric meaning of the Hessian matrix. Also, we will discuss the eigenvalues and eigenvectors of the Hessian and introduce the application of it.&lt;/p&gt;

&lt;p&gt;This post was written with reference to the following materials.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://angeloyeo.github.io/2020/06/17/Hessian.html&quot;&gt;Donghoon Yeo‚Äôs blog posting&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Hessian_matrix&quot;&gt;Wikipedia &amp;gt; Hessian&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;hessian-matrix&quot;&gt;Hessian Matrix&lt;/h1&gt;

&lt;h2 id=&quot;definition&quot;&gt;Definition&lt;/h2&gt;
&lt;p&gt;In mathematics, the Hessian matrix or Hessian is a square matrix of second-order partial derivatives of a scalar-valued function, or scalar field. It describes the local curvature of a function of many variables. [1]&lt;/p&gt;

&lt;p&gt;Suppose $f : ‚Ñù^n ‚Üí ‚Ñù$ is a function taking as input a vector $x ‚àà ‚Ñù^n$ and outputting a scalar $f(x) ‚àà ‚Ñù$. If all second partial derivatives of $f$ exist and are continuous over the domain of the function, then the Hessian matrix $H$ of $f$ is a square n√ón matrix, usually defined and arranged as follows; [1]&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2020-07-21-geometric meaning of hessian/hessian.png&quot; width=&quot;60%&quot; /&gt;   &lt;figcaption&gt;
Figure 1. Definition of Hessian matrix. [1]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;geometric-meaning&quot;&gt;Geometric Meaning&lt;/h2&gt;

&lt;p&gt;First of all, it is well known that &lt;strong&gt;all matrices can be considered as linear transformations&lt;/strong&gt;. Likewise, we can think of &lt;strong&gt;a Hessian matrix as a linear transformation&lt;/strong&gt;. Geometrically, the main feature of the linear transformation performed by the Hessian matrix is &lt;strong&gt;to make a given function more convex or concave&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Let‚Äôs look at the Hessian‚Äôs geometric meaning in detail through examples.
The Hessian matrices corresponding to Figure 2 and 3 are as follow;&lt;/p&gt;

\[\begin{bmatrix}2 &amp;amp; 1\\1 &amp;amp; 2\\\end{bmatrix} and \begin{bmatrix}2 &amp;amp; 0\\0 &amp;amp; -2\\\end{bmatrix}.\]

&lt;p&gt;The things to note here are, &lt;strong&gt;the eigenvectors of the Hessian matrix represent the principal axis of transformation and the eigenvalues represent the degree of transformation.&lt;/strong&gt; More specifically, if the eigenvalues are all positive (Figure 2), it makes the given function more convex. Conversely, if the eigenvalues are all negative, it makes the given function more concave. However, if some of the eigenvalues are positive, and some are negative (Figure 3), the given function is converted to a saddle shape. To recapitulate, &lt;strong&gt;by using the main feature of the geometric transformation shown by the Hessian, we could emphasize the gradient change of the given function.&lt;/strong&gt;&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2020-07-21-geometric meaning of hessian/hessian gif 01.gif&quot; width=&quot;90%&quot; /&gt;   &lt;figcaption&gt;
Figure 2. Example of Hessian transform making the given function more convex. [2]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2020-07-21-geometric meaning of hessian/hessian gif 02.gif&quot; width=&quot;90%&quot; /&gt;   &lt;figcaption&gt;
Figure 3. Example of Hessian transform making the given function to a saddle shape function. [2]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;applications&quot;&gt;Applications&lt;/h2&gt;

&lt;p&gt;Using the feature that the Hessian emphasizes the gradient change, it can be used as a filter that detects the contour (edge) of a specific object in a given image, which is considered as a function at this point. Figure 4 shows the description of edge detection and the Hessian‚Äôs eigenvalues, and Figure 5 shows an application detecting the vessel using the Hessian.&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2020-07-21-geometric meaning of hessian/application 02.png&quot; width=&quot;60%&quot; /&gt;   &lt;figcaption&gt;
Figure 4. Edge detection using Hessian's eigenvalues [3]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure align=&quot;center&quot;&gt;
&lt;img src=&quot;https://jhyun0919.github.io/assets/img/2020-07-21-geometric meaning of hessian/application 01.png&quot; width=&quot;60%&quot; /&gt;   &lt;figcaption&gt;
Figure 5. Hessian's applicaiton: Frangi filter for vessel detection. [2]
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] ‚ÄúHessian matrix,‚Äù Wikipedia, 24-Jun-2020. [Online]. Available: https://en.wikipedia.org/wiki/Hessian_matrix. [Accessed: 21-Jul-2020].&lt;/p&gt;

&lt;p&gt;[2] Y. D. Yeo, ‚ÄúGeometrical meaning of Hessian Matrix,‚Äù 17-Jun-2020. [Online]. Available: https://angeloyeo.github.io/2020/06/17/Hessian.html. [Accessed: 21-Jul-2020].&lt;/p&gt;

&lt;p&gt;[3] ‚ÄúHessian Matrix of the image,‚Äù Stack Overflow, 01-Sep-1963. [Online]. Available: https://stackoverflow.com/questions/22378360/hessian-matrix-of-the-image. [Accessed: 21-Jul-2020].&lt;/p&gt;
</description>
        <pubDate>Tue, 21 Jul 2020 00:00:00 -0500</pubDate>
      </item>
    
  </channel>
</rss>
