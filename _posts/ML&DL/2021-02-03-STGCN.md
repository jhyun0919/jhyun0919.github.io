---
layout: post
title: (Paper Review) Spatio-Temporal Graph Convolutional Networks
categories: [Research/ML&DL]
tags: [GCN, STGCN, Research Proj]
---

#### TL; DR


<br>

---

# 1. INTRODUCTION

## 1.1. What is the problem?

Transportation plays a vital role in everybody‚Äôs daily life. According to a survey in 2015, U.S. drivers spend about 48 minutes on average behind the wheel daily [2]. Under this circumstance, an accurate real-time forecast of traffic conditions is of paramount importance for road users, private sectors, and governments.

There were many efforts to monitor the current status of traffic conditions and to predict the future. These studies aimed to contribute to widely used transportation services, such as flow control, route planning, and navigation.

<br>

## 1.2. History of previous approaches

Previous studies on traffic prediction can be roughly divided into two categories: **Dynamical modeling** and **Data-driven methods**.

Many researchers are shifting their attention to data-driven approaches, since dynamical modeling methods have drawbacks as follow. üò°

- It requires sophisticated systematic programming.

- It consumes massive computational power.

- Impractical assumptions and simplifications among the modeling degrade the prediction accuracy.

<br>

As a result, **Deep learning approaches**, which are major representatives of data-driven approaches, have been widely and successfully applied to various traffic tasks. Significant progress has been made in following works. üòÅ

- Deep belief network (DBN) [Jia et al., 2016; Huang et al., 2014]

- Stacked auto-encoder (SAE) [Lv et al., 2015; Chen et al., 2016]

However, it is difficult for these dense networks to extract spatial and temporal features from the input jointly. üò°

<br>

To take full advantage of spatial features, some researchers use **convolutional neural network (CNN)** to capture adjacent relations among the traffic network, along with employing **recurrent neural network (RNN)** on time axis. üòÅ

However, this approach has limitations as follow. üò°

- The normal convolutional operation applied restricts the model to only process grid structures (e.g. images, videos) rather than general domains.

- Rrecurrent networks for sequence learning require iterative training, which introduces error accumulation by steps.

- RNN-based networks are widely known to be difficult to train and computationally heavy.

<br>

## 1.3. Proposed Approach

For overcoming the limitations of previous approaches, the author introduces several strategies to effectively model temporal dynamics and spatial dependencies of traffic flow. üòÅ

- To fully utilize spatial information, the author models the traffic network by a general graph instead of treating it separately (e.g. grids or segments).

- To handle the inherent deficiencies of recurrent networks, the author employs a fully convolutional structure on time axis.

Above all, the author proposes a novel deep learning architecture, **the spatio-temporal graph convolutional networks (STGCN)**, for traffic forecasting tasks. ü§ò
<!-- This architecture comprises several spatio-temporal convolutional blocks, which are a combination of graph convolutional layers [Defferrard et al., 2016] and convolutional sequence learning layers, to model spatial and temporal dependencies. -->

<span style="color:green">
Q. IS THERE ANY DRAWBACK???
</span>

<br>

---

# 2. PRELIMINARY
## 2.1. Traffic Prediction on Road Graphs
<br>

#### Traffic forecast as a typical time-series prediction problem

Predicting the most likely traffic measurements (e.g. speed or traffic flow) in the next $H$ time steps given the previous $M$ traffic observations as,

$$
\hat{v}_{t+1}, ..., \hat{v}_{t+H} = \underset{v_{t+1}, ..., v_{t+H}}{\mathrm{argmax}}\log{P(v_{t+1}, ..., v_{t+H} | v_{t-M+1}, ..., v_{t})}

\tag{1}
$$

where $v_t \in \mathbb{R}^{n}$ is an observation vector of $n$ road segments at time step $t$, each element of which records historical observation for a single road segment.

<br>

#### Graph structured trafic time series problem

In this work, the author defines the **traffic network on a graph** and focuses on **structured traffic time series**. The traffic network is represented as an undirected graph (or directed one) $\mathcal{G_t}$ with weights $w_{ij}$ as shown in Figure 1.

<figure align="center">
  <img src="https://jhyun0919.github.io/assets/img/2021-02-03-STGCN/graph_traffic_data.png" width="400" />
  <figcaption>Figure 1.  Graph-structured traffic data. [1]</figcaption>
</figure>

<br>

At the $t$-th time step, in graph $\mathcal{G}_t = (\mathcal{V}_t, \mathcal{E}, \mathcal{W})$,

- $\mathcal{V}_t$ is a finite set of vertices, corresponding to the observations from $n$ monitor stations at time $t$.

- $\mathcal{E}$ is a set of edges, indicating the connectedness between stations.

- $\mathcal{W} \in \mathbb{R}^{n \times n}$ denotes the weighted adgacency matrix of $\mathcal{G}_t$.

<span style="color:green">
Q. DOES EDGES CAN HAVE ITS OWN TIME VARIANT VARIABLES??? e.g. temperature of transmission lines in power sys...
</span>

<br>

## 2.2. Convolutions on Graphs

A standard convolution for regular grids, such as images and video, is clearly not applicable to general graphs. There are two basic approaches currently exploring how to generalize CNNs to structured data forms.

- Expanding the **spatial** definition of a convolution [Niepert et al., 2016]

- Manipulating in the **spectral** domain with graph Fourier transforms [Bruna et al., 2013]

The first approach, using spatial convolution, rearranges the vertices into certain grid forms which can be processed by normal convolutional operations.
The second approach introduces the spectral framework to apply convolutions in spectral domains, often named as the spectral graph convolution.

<span style="color:green">
Q. PROS & CONS OF SPATIAL & SPECTRAL???
</span>

<br>

In this paper, the author introduces the notion of graph convolution operator "$*\mathcal{G}$"  based on the conception of spectral graph convolution, as the multiplication of a signal $x \in \mathbb{R^n}$ with a kernel $\Theta$,

<span style="color:green">
Q. WHY AUTHOR CHOOSE SPECTRAL APPROACHES???
</span>

$$
\Theta *\mathcal{G} x = \Theta (L) x = \Theta (U \Lambda U^T) x = U \Theta (\Lambda) U^T x
\tag{2}
$$

where

- $U \in \mathbb{R}^{n\times n}$, Graph Fourier basis, is the matrix of
eigenvectors of the normalized graph Laplacian $L$.

- $L$ is the normalized graph Laplacian matrix.

- $L = I_n - D^{-\frac{1}{2}} W D^{-\frac{1}{2}} = U \Lambda U^T \in \mathbb{R}^{n\times n}$ *(check details in appendix 6.1)*

  - $I_n$ is an identity matrix.

  - $W$ is the weighted adgacency matrix, which is symmetric.

  - $D \in \mathbb{R}^{n\times n}$ is the diagonal degree matrix with $D_{ii} = \sum_{j}{W_{ij}}$.

  - $\Lambda \in \mathbb{R}^{n\times n}$ is the diagonal matrix of eigenvalues of $L$.

By this definition, a graph signal $x$ is filtered by a kernel $\Theta$ with multiplication between $\Theta$and graph Fourier transform $U^T x$ [Shuman et al., 2013].

<span style="color:green">
Q. HOW CAN WE COMPUTE O(n^2)???
</span>

<br>

---

# 3. PROPOSED MODEL

## 3.1. Network Architecture

The author proposed a model architecture called spatio-temporal graph convolutional networks (STGCN). As shown in Figure 2, STGCN is composed of several spatio-temporal convolutional blocks.
Each spatio-temporal convolutional blocks is formed as a ‚Äúsandwich‚Äù structure with two gated sequential convolution layers and one spatial graph convolution layer in between.

<span style="color:green">
Q. WHY IT HAS A SANDWICH STRUCTURE???
</span>

<figure align="center">
  <img src="https://jhyun0919.github.io/assets/img/2021-02-03-STGCN/stgcn_architecture.png" width="666" />
  <figcaption>Figure 2.  Architecture of spatio-temporal graph convolutional networks (STGCN). [1]</figcaption>
</figure>

<br>

## 3.2. Graphs CNNs for Extracting Spatial Features

The traffic network generally organizes as a graph structure. Since 2-D convolutions on grids can only capture the spatial locality, attributes of traffic networks (graph structured data) were not fully extracted by 2-D convolution filters.

Accordingly, in STGCN, the graph convolution is employed directly on graph-structured data to extract highly meaningful patterns and features in the space domain.

Though the computation of kernel $\Theta$ in graph convolution by Eq. (2) can be expensive due to $\mathcal{O}(n^2)$ multiplications with graph Fourier basis. üò°

However, the author applied two approximation strategies to overcome this issue. üòÅ

<br>

#### Chebyshev Polynomials Approximation

Chebyshev polynomial $T_k(x)$ is used to approximate kernels as a truncated expansion of order $K-1$ as $\Theta (\Lambda) \approx \sum_{k=0}^{K-1} \theta_k T_k(\tilde{\Lambda})$ with rescaled $\tilde{\Lambda} = 2 \Lambda/\lambda_{max}-I_n$ ($\lambda_{max}$ denotes the largest eigenvalues of $L$).

<span style="color:green">
Q. RESCALED??? WHY???
</span>

Using the Chebyshev polynomials approximation, the graph convolution can then be rewritten as,

$$
\begin{aligned} \Theta*\mathcal{G}x  &= \Theta(L)x \\ &\approx \sum_{k=0}^{K-1} \theta_k T_k(\tilde{L})x \end{aligned}
\tag{3}
$$

where $T_k(\tilde{L}) \in \mathbb{R}^{n \times n}$ is the Chebyshev polynomial of order $k$ evaluated at the scaled Laplacian $\tilde{L} = 2L / \lambda_{max} - I_n$.

As a result, we can localize the filter and reduce the number of parameters by restricting kernel $\Theta$ to a polynomial of order $k$.

Moreover, by recursively computing $K$-localized convolutions through the polynomial approximation, the cost of Eq. (2) can be reduced to $\mathcal{O}(K\|\mathcal{E}\|)$ as Eq. (3) shows [Defferrard et al., 2016]. *(check details in appendix 6.3)*

<br>

#### 1st-order Approximation

A layer-wise linear formulation can be defined by stacking multiple localized graph convolutional layers with the first-order approximation of graph Laplacian [Kipf and Welling, 2016].

Consequently, a deeper architecture can be constructed to recover spatial information in depth without being limited to the explicit parameterization given by the polynomials.

Due to the scaling and normalization in neural networks, we can further assume that $\lambda_{max} \approx 2$.

Thus, the Eq. (3) can be simplified to,

$$
\begin{aligned} \Theta*\mathcal{G}x & \approx \theta_0x + \theta_1(\frac{2}{\lambda_{max}}L-I_n)x \\ &\approx \theta_0x - \theta_1(D^{-1/2}WD^{-1/2})x \end{aligned}
\tag{4}
$$

where $\theta_0$, $\theta_1$ are two shared parameters of the kernel.


In order to constrain parameters and stabilize numerical performances, $\theta_0$ and $\theta_1$ are replaced by a single parameter $\theta$ by letting $\theta = \theta_0 = -\theta_1$; $W$ and $D$ are renormalized by $\tilde{W}=W+I_n$ and $\tilde{D}_{ii}=\sum_{j}\tilde{W}_{ij}$ separately.

Then, the graph convolution can be alternatively expressed as,

$$
\begin{aligned} \Theta*\mathcal{G}x &= \theta(I_n + D^{-1/2}WD^{-1/2})x \\&= \theta(\tilde{D}^{-1/2}\tilde{W}\tilde{D}^{-1/2})x \end{aligned} \tag{5}
$$

Applying a stack of graph convolutions with the 1st-order approximation vertically that achieves the similar effect as $K$-localized convolutions do horizontally, all of which exploit the information from the $(K-1)$-order neighborhood of central nodes.

In this scenario, $K$ is the number of successive filtering operations or convolutional layers in a model instead.

Additionally, the layer-wise linear structure is parameter-economic and highly efficient for large-scale graphs, since the order of the approximation is limited to one.

<br>


#### Generalization of Graph Convolutions

The graph convolution operator "$*\mathcal{G}$" defined on $x \in \mathbb{R}^n$ can be extended to multi-dimensional tensors.

For a signal $C_i$ channels $X \in \mathbb{R}^{n \times C_i}$, the graph convolution can be generalized by,

$$
y_i = \sum_{i=1}^{C_i} \Theta_{ij}(L)x_i \in \mathbb{R}^n, 1 \leq j \leq C_o \tag{6}
$$

with the $C_i \times C_o$ vectors of Chebyshev coefficients $\Theta_{ij} \in \mathbb{R}^K$ ($C_i$, $C_o$ are the size of input and output of the feature maps, respectively).

The graph convolution for 2-D variables is denoted as "$\Theta *\mathcal{G}X$" with $\Theta \in \mathbb{R}^{K \times C_i \times C_o}$.

<br>

## 3.3. Gated CNNs for Extracting Temporal Features

RNNs for traffic prediction still suffer from 

- time-consuming iterations
- complex gate mechanisms
- slow response to dynamic changes.

On the contrary, CNNs have the superiority of 

- fast training
- simple structures
- no dependency constraints to previous steps.

we employ entire convolutional structures on time axis to capture temporal dynamic behaviors of traffic flows. 

This specific design allows parallel and controllable training procedures through multi-layer convolutional structures formed as hierarchical representations.


<figure align="center">
  <img src="https://jhyun0919.github.io/assets/img/2021-02-03-STGCN/temporal_gated_conv.png" width="273" />
  <figcaption>Figure 3. Temporal Gated-Conv Block. [1]</figcaption>
</figure>

<br>


For each node in graph $\mathcal{G}$, the temporal convolution explores $K_t$ neighbors of input elements without padding which leading to shorten the length of sequences by $K_{t}-1$ each time. Thus, input of temporal convolution for each node can be regarded as a length-$M$ sequence with $C_i$ channels as $Y \in \mathbb{R}^{M \times C_i}$.

The convolution kernel $\Gamma \in \mathbb{R}^{K_t \times C_i \times 2C_o}$ is designed to map the input $Y$ to a single output element $[P \: Q] \in \mathbb{R}^{(M-K_t+1) \times (2C_o)}$ ($P, Q$ is split in half with the same size of channels).

As a result, the temporal gated convolution can be defined as,

$$
\Gamma * \tau Y=P \odot \sigma (Q) \in \mathbb{R}^{(M-K_t+1) \times C_o} \tag{7}
$$

where

- $P, Q$ are input of gates in GLU respectively

- $\odot$ denotes the element-wise Hadamard product.

  - Hadamard product (also known as the element-wise, entry wise or Schur product)

- $\sigma$ is sigmoid gate controls which input $P$ of the current states are relevant for discovering compositional structure and dynamic variances in time series.


The non-linearity gates contribute to the exploiting of the full input filed through stacked temporal layers as well. Furthermore, residual connections are implemented among stacked temporal convolutional layers. Similarly, the temporal convolution can also be generalized to 3-D variables by employing the same convolution kernel \Gamma to every node $\mathcal{Y}_i \in \mathbb{R}^{M \times C_i}$ (e.g. sensor stations) in $\mathcal{G}$ equally, noted as "$\Gamma * \tau \mathcal{Y}$" with $\mathcal{Y} \in \mathbb{R}^{M \times n \times C_i}$.


<br>

## 3.4. Spatio-Temporal Convolutional Block

In order to fuse features from both spatial and temporal domains, the spatio-temporal convolutional block (ST-Conv block) is constructed to jointly process graph-structured time series.

The block itself can be stacked or extended based on the scale and complexity of particular cases.

<figure align="center">
  <img src="https://jhyun0919.github.io/assets/img/2021-02-03-STGCN/st_conv_block.png" width="273" />
  <figcaption>Figure 4. Spatio-Temporal Convolutional (ST-Conv) Block. [1]</figcaption>
</figure>

<br>

As illustrated in Figure 2 (mid), the spatial layer in the middle is to bridge two temporal layers which can achieve fast spatial-state propagation from graph convolution through temporal convolutions.

The ‚Äúsandwich‚Äù structure also helps the network sufficiently apply bottleneck strategy to achieve scale compression and feature squeezing by downscaling and upscaling of channels $C$ through the graph convolutional layer.

Moreover, layer normalization is utilized within every ST-Conv block to prevent overfitting.

The input and output of ST-Conv blocks are all 3-D tensors. For the input $v^l \in \mathbb{R}^{M \times b \times C^l}$ of block $l$, the output $v^{l+1} \in \mathbb{R}^{(M-2(K_t-1)) \times n \times C^{l+1}}$is computed by,

$$
v^{l+1} = \Gamma_1^l * \tau ReLU(\Theta^l * \mathcal{G}(\Gamma_0^l * \tau v^l)) \tag{8}
$$

where

- $\Gamma_0^l$, $\Gamma_1^l$ are the upper and lower temporal kernel within block $l$, respectively.

- $\Theta^l$ is the spectral kernel of graph convolution

- $ReLU(\cdot)$ denotes the rectified linear units function.



<figure align="center">
  <img src="https://jhyun0919.github.io/assets/img/2021-02-03-STGCN/st_convs_fcn.png" width="273" />
  <figcaption>Figure 5. The framework STGCN consists of two ST-Conv blocks and a fully-connected output layer in the end. [1]</figcaption>
</figure>

<br>

We use L2 loss to measure the performance of our model. Thus, the loss function of STGCN for traffic prediction can be written as,

$$
L(\hat{v};W_{\theta})=\sum_{t}\| \hat{v}(v_{t-M+1}, ..., v_t, W_\theta) - v_{t+1} \|^2 \tag{9}
$$

where

- $W_\theta$ are all trainable parameters in the model

- $v_{t+1}$ is the ground truth

- $\hat{v}(\cdot)$ denotes the model‚Äôs prediction




We now summarize the main characteristics of our model STGCN in the following,

- STGCN is a universal framework to process structured time series. It is not only able to tackle traffic network modeling and prediction issues but also to be applied to more general spatio-temporal sequence learning tasks.

- The spatio-temporal block combines graph convolutions and gated temporal convolutions, which can extract the most useful spatial features and capture the most essential temporal features coherently.

- The model is entirely composed of convolutional structures and therefore achieves parallelization over input with fewer parameters and faster training speed. More importantly, this economic architecture allows the model to handle large-scale networks with more efficiency.


<br>

---

# 4. EXPERIMENTS & RESULTS





<br>

## 4.1. Dataset Description

**BJER4** was gathered from the major areas of east ring No.4 routes in Beijing City by double-loop detectors. There are 12 roads selected for our experiment. The traffic data are aggregated every 5 minutes. The time period used is from 1st July to 31st August, 2014 except the weekends. We select the first month of historical speed records as training set, and the rest serves as validation and test set respectively.

**PeMSD7** was collected from Caltrans Performance Measurement System (PeMS) in real-time by over 39,000 sensor stations, deployed across the major metropolitan areas of California state highway system [Chen et al., 2001]. The dataset is also aggregated into 5-minute interval from 30-second data samples. We randomly select a medium and a large scale among the District 7 of California containing 228 and 1,026 stations, labeled as **PeMSD7(M)** and **PeMSD7(L)**, respectively, as data sources (shown in the left of Figure 3). The time range of PeMSD7 dataset is in the weekdays of May and June of 2012. We split the training and test sets based on the same principles as above.

<figure align="center">
  <img src="https://jhyun0919.github.io/assets/img/2021-02-03-STGCN/PeMS_PeMSD7.png" width="666" />
  <figcaption>Figure 6. PeMS sensor network in District 7 of California (left), each dot denotes a sensor station; Heat map of weighted adjacency matrix in PeMSD7(M) (right). [1]</figcaption>
</figure>

<br>

## 4.2. Data Preprocessing



$$

$$

<br>

## 4.3. Experimental Settings





<br>

## 4.4. Experimental Results


#### Benefits of Spatial Topology

#### Training Efficiency and Generalization

<figure align="center">
  <img src="https://jhyun0919.github.io/assets/img/2021-02-03-STGCN/table1.png" width="600" />
  <figcaption>Table 1. Performance comparison of different approaches on the dataset BJER4. [1]</figcaption>
</figure>

<br>





<figure align="center">
  <img src="https://jhyun0919.github.io/assets/img/2021-02-03-STGCN/table2.png" width="800" />
  <figcaption>Table 2. Performance comparison of different approaches on the dataset PeMSD7. [1]</figcaption>
</figure>

<br>





<figure align="center">
  <img src="https://jhyun0919.github.io/assets/img/2021-02-03-STGCN/result1.png" width="666" />
  <figcaption>Figure 7. Speed prediction in the morning peak and evening rush hours of the dataset PeMSD7. [1]</figcaption>
</figure>

<br>





<figure align="center">
  <img src="https://jhyun0919.github.io/assets/img/2021-02-03-STGCN/result2.png" width="666" />
  <figcaption>Figure 8. Test RMSE versus the training time (left); Test MAE ver- sus the number of training epochs (right). (PeMSD7(M)) [1]</figcaption>
</figure>

<br>




<figure align="center">
  <img src="https://jhyun0919.github.io/assets/img/2021-02-03-STGCN/table3.png" width="500" />
  <figcaption>Table 3. Time consumptions of training on the dataset PeMSD7. [1]</figcaption>
</figure>

<br>






<br>

---

# 5. CONCLUSION & FUTURE WORK

In this paper,

- we propose a novel deep learning framework STGCN for traffic prediction
- integrating graph convolution and gated temporal convolution through spatio-temporal convolutional blocks.

Experiments show that our model outperforms other state-of-the-art methods on two real-world datasets, indicating its great potentials on exploring spatio-temporal structures from the input.

It also achieves

- faster training
- easier convergences
- fewer parameters with flexibility and scalability

These features are quite promising and practical for scholarly development and large-scale industry deployment. In the future, we will further optimize the network structure and parameter settings.

Moreover, our proposed framework can be applied into more general spatio-temporal structured sequence forecasting scenarios, such as evolving of social networks, and preference prediction in recommendation systems, etc.


<br>

---

# 6. APPENDIX

## 6.1. Laplacian & Symmetric normalized Laplacian []

<br>

#### Laplacian

Given a simple graph $\mathcal{G}$ with $n$ vertices, its Laplacian matrix $L_{n \times n}$ is defined as,

$$
L = D - A
$$

where

- $D$ is the degree matrix of the graph.

- $A$ is adjacency matrix.

The elements of $L$ are given by

$$
L_{ij}:= \begin{cases} deg(v_i) &\text{if } i=j \\ -1 &\text{if } i\neq j \text{ and } v_i \text{ is adjacent to } v_j \\ 0 &\text{otherwise} \end{cases}
$$

where

- $deg(v_i)$ is the degree of the vertex $i$.

<br>

#### Symmetric normalized Laplacian

The symmetric normalized Laplacian matrix is defined as,

$$
L = I - D^{-1/2} A D^{-1/2}
$$

The elements of $L^{sym}$ are given by

$$
L_{ij}:=
\begin{cases}
1 &\text{if } i=j \text{ and } deg(v_i) \neq 0\\
-\frac{1}{\sqrt{deg(v_i)deg(v_j)}} &\text{if } i\neq j \text{ and } v_i \text{ is adjacent to } v_j \\
0 &\text{otherwise}
\end{cases}
$$

<figure align="center">
  <img src="https://jhyun0919.github.io/assets/img/2021-02-03-STGCN/laplacian_matrix.png" width="666" />
  <figcaption>Figure 9. Example of a General Graph and its Degree, Adjacency, and Laplacian Matrix. []</figcaption>
</figure>

<br>

## 6.2 Graph Fourier transform []

Graph Fourier transform is a mathematical transform which eigendecomposes the Laplacian matrix of a graph into eigenvalues and eigenvectors. Analogously to classical Fourier Transform, the eigenvalues represent frequencies and eigenvectors form what is known as a graph Fourier basis.

$\mathcal{O}(n^2)$

<br>

---
# Reference

[1] B. Yu, H. Yin, and Z. Zhu, ‚ÄúSpatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting,‚Äù arXiv.org, 12-Jul-2018.

[2] B. Tefft, S. Rosenbloom, R. Santos, and T. Triplett, ‚ÄúAmerican Driving Survey: 2014 ‚Äì 2015,‚Äù AAA Foundation, 14-Jun-2018. [Online]. Available: https://aaafoundation.org/american-driving-survey-2014-2015/. [Accessed: 15-Feb-2021].



[] ‚ÄúLaplacian matrix,‚Äù Wikipedia, 10-Jan-2021. [Online]. Available: https://en.wikipedia.org/wiki/Laplacian_matrix. [Accessed: 03-Feb-2021].

[] ‚ÄúGraph Fourier Transform,‚Äù Wikipedia, 30-Dec-2020. [Online]. Available: https://en.wikipedia.org/wiki/Graph_Fourier_Transform. [Accessed: 01-Mar-2021].